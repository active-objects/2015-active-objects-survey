% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\usepackage{tikz}
\usepackage{tabularx}

\usepackage{parcolumns}
\usepackage{listings,multicol,amssymb}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\usepackage{listings}
\newcommand{\code}[1]{\texttt{\small{#1}}}
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{[TODO:#1]}}}
\newcommand{\NOTE}[1]{\textcolor{blue}{\textbf{[Note:#1]}}}
\newcommand{\LUDO}[1]{\textcolor{blue}{\textbf{[Ludo:#1]}}}
\newcommand{\KIKO}[1]{\textcolor{cyan}{\textbf{[Kiko:#1]}}}
\newcommand{\TW}[1]{\textcolor{magenta}{\textbf{[Tobias:#1]}}}
\newcommand{\parT}{\texttt{ParT}}

%% For bold syntax highlighting in the Encore chapter
\renewcommand{\ttdefault}{lmtt}
\DeclareFontFamily{T1}{lmtt}{}
\DeclareFontShape{T1}{lmtt}{m}{n}{<-> ec-lmtl10}{}
\DeclareFontShape{T1}{lmtt}{m}{\itdefault}{<-> ec-lmtlo10}{}
\DeclareFontShape{T1}{lmtt}{\bfdefault}{n}{<-> ec-lmtk10}{}
\DeclareFontShape{T1}{lmtt}{\bfdefault}{\itdefault}{<-> ec-lmtko10}{}


% Metadata Information
%\acmVolume{9}
%\acmNumber{4}
%\acmArticle{39}
%\acmYear{2010}
%\acmMonth{3}


% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\doi{0000001.0000001}

%ISSN
%\issn{1234-56789}

% Document starts
\begin{document}

% Page heads
%\markboth{G. Zhou et al.}{A Multifrequency MAC Specially Designed for WSN Applications}

% Title portion
\title{A Survey of Active Objects and Actors}
\author{To fill
\affil{To fill}
To fill
\affil{To fill}
To fill
\affil{To fill}
To fill
\affil{To fill}
To fill
\affil{To fill}
To fill
\affil{To fill}
To fill
\affil{To fill}}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block 
%(below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he 
%is
%       currently affiliated with NASA.

\begin{abstract}
the abstract
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%
% End generated code
%

% We no longer use \terms command
%\terms{Design, Algorithms, Performance}

\keywords{Active objects, actors, concurrency, distributed systems}

%\acmformat{Gang Zhou, Yafeng Wu, Ting Yan, Tian He, Chengdu Huang, John A. Stankovic,
%and Tarek F. Abdelzaher, 2010. A multifrequency MAC specially
%designed for  wireless sensor network applications.}
% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author 
%names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 
%'auto-generated'.

%\begin{bottomstuff}
%This work is supported by the National Science Foundation, under
%grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.
%
%Author's addresses: G. Zhou, Computer Science Department,
%College of William and Mary; Y. Wu  {and} J. A. Stankovic,
%Computer Science Department, University of Virginia; T. Yan,
%Eaton Innovation Center; T. He, Computer Science Department,
%University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
%(Current address) NASA Ames Research Center, Moffett Field, California 94035.
%\end{bottomstuff}

\maketitle


\TODO{TODO LIST}



\NOTE{Page limit is 35 pages}


\section{Introduction}

A global introduction to the common problems in programming distributed systems. Safety 
issues, common bugs and efficiency issues. A first overview of the families of 
programming models for concurrent and distributed systems. What is difficult? What 
features are important in concurrent? in distributed systems?


\section{Active object Languages}

\subsection{An historical view of Actor and active-object languages}

We present the languages discussed in this overview paper in a
historical context.  The general development of high-level programming
languages is driven by the quest for suitable abstractions of the
low-level implementation details of the specific underlying hardware
architectures, i.e., machine instructions.  The basic abstraction of
the machine instructions for reading from and writing to memory
locations which underlies imperative programming is the
assignment. The low-level flow of control is further abstracted by
means of high-level structuring mechanisms which allow the
construction of a program in a compositional manner, e.g., sequential
composition, choice and iterative constructs.

One of the first major high-level imperative programming languages is
ALGOL (short for ALGOrithmic Language) \cite{backus63cacm} which
features procedural and functional abstractions of data and control.
The resulting general abstraction level allows for a compositional
construction and analysis of mainly the flow of control.  A major
advance in the development of high-level programming languages which
gave rise to the paradigm of object-orientation is the compositional
construction of programs in terms of modules.  Modules are used to
model abstract data types and as such provide a powerful abstraction
which fully integrates data and control.  In the object-oriented
programming paradigm modules are generalized to classes.  Classes can
be characterized as modules from which objects can be dynamically
instantiated and referred to by unique identifiers (generated at
runtime).  The concepts of classes and objects were introduced in
Simula 67 \cite{dahl66cacm}, the first object-oriented
language. Whereas Simula was a sequential language with a single
thread of control, it was developed to simulate real-world
(concurrent) systems and featured the concept of \emph{coroutines}
\cite{dahl68simula}.

A major remaining challenge in the development of high-level
programming languages is the high-level specification of concurrent
and parallel threads of control and their synchronization.  In process
calculi like CSP \cite{Hoare85} and CCS \cite{Milner89}
independent threads of control communicate data synchronously, e.g.,
via channels.  The Ada programming language \cite{bal89acmsurv}
extends this model of synchronous communication by a rendez-vous
interpretation of the execution of procedure calls and returns.  In
the shared-variable model of concurrency, threads of control
communicate and synchronize directly via shared data. For example in
the popular object-oriented language Java \cite{goetz06java} such a
thread of control consists of a stack of procedure (method) calls,
extending the sequential thread of control of Simula and Smalltalk
from one to multiple threads in an manner resembling operating
systems.

The shared-variable model of concurrency underlying the Java language
requires that the developer ensures so-called ``thread safety'';
without enough synchronization data races between threads make the
program behavior ``unpredictable and sometimes surprising''
\cite{goetz06java}. To avoid races, complex low-level synchronization
mechanisms are required which may in turn give rise to deadlocks and
thread starvation.  The execution of multithreaded programs depends on
an underlying, intricate, and fine-grained granularity of
interleaving, which makes multithreaded programs very difficult to
understand and verify.  Further, since data, as represented statically
by classes, and control, as represented at run-time by threads, are
decoupled, this model of concurrency is also unintuitive and
difficult to use in a distributed setting.

In contrast to the above synchronous communication mechanisms, the
Actor model of computation \cite{Agha86-book} is based on loosely
coupled processes and asynchronous message passing.  Actors integrate
and encapsulate both data and a single thread of control, and
communicate without transfer of control. Message delivery is
guaranteed but messages may arrive out of order.  This loose coupling
makes Actor languages conceptually attractive for distributed
programming.  Active objects integrate the basic Actor model with
object-orientated concepts. This integration can be realized by
modeling a message as an asynchronous method call, thus fixing the
interpretation of a message by a corresponding method definition. 
 In
 Rebeca \cite{DBLP:conf/csicc/SirjaniMM01,DBLP:journals/jucs/SirjaniBM05} 
the methods are run to completion without any
further synchronization mechanism and do not return values. Like in
the original Actor model, returning values from a method call can be
modeled by another asynchronous message. 
%Rebeca is an 
%actor-based language with a FIFO point-to-point communication and message service. 
%Actors are encapsulated units of concurrency, with no shared variables. Communication takes place by asynchronous message passing, with non-blocking send and receive statements. 
%In Rebeca, computation is message-driven, a message is taken from the top of the message 
%queue and is executed non-preemptively. In addition, instead of having a soup of 
%messages 
%like in Agha's actor model, 
However,  in Rebeca the order of messages between any two actors is 
preserved. 
%A network where messages can be lost or arrive out-of-order, can be modeled using an 
%actor that behaves like the intermediate medium. The same as the original actor mode, 
%``while'' loops or periodic events are modeled by sending messages to itself. 
 % Based on the application 
%domains, where necessary, Rebeca is extended to provide other means of communication or 
%safe synchronization.
 In contrast, in the
 Actor-based extension of Scala \cite{haller2009scala}, messages are
 interpreted by the callee using pattern-matching.
 
Rebeca is 
designed to be simple and analyzable. Efficient compositional verification 
\cite{DBLP:journals/jucs/SirjaniMSB05} and specific state space reduction techniques 
\cite{DBLP:journals/acta/JaghooriSMKM10}  are created based on the following 
characteristics of the model: no shared variable, no blocking send or receive, 
single-threaded actors, and non-preemptive execution of each message (execution of a 
message  will not 
interfere with  execution of another one).

Pushing the notion of method calls further by directly supporting
return values leads to the notion of future variables.  Futures were
originally discovered by Baker and Hewitt in the
70s~\cite{BakerHH77}, and later rediscovered by Liskov and
Shrira as promises in Argus~\cite{Liskov88} and by Halstead in the
context of MultiLisp~\cite{Halstead:1985:MLC:4472.4478} before finding their way
into object-oriented languages such as
Concurrent\-Smalltalk~\cite{Yokote87}, ABCL~\cite{Yonezawa:1986:OCP:28697.28722},
Eiffel$/\!/$~\cite{Caromel96}, and CJava~\cite{Cugola97}. A future may
be understood as a mailbox which will eventually contain the return
values from a method call. Thus the calling method may proceed with
its computation and pick up the reply later. However, this requires
additional synchronization by means of a blocking get-operation on the
future.

In Creol \cite{johnsen03nik,johnsen07sosym} a powerful further
synchronization mechanism is provided by cooperative scheduling of the
methods of an active object by an explicit statement which allows the
release of control.  This gives rise to a coroutine like execution of
the method activations of an active object which encapsulates a single
thread of control. This model of computation supports a compositional
proof theory \cite{deBoerDJ07}.


Caromel, Henrio, and Serpette~\cite{CHS:POPL04} present ASP, an
imperative, asynchronous object calculus with transparent futures.
Their active objects may have internal passive objects which can be
passed between active objects by first deep copying the entire
(passive) object graph.  To manage the complexity of reasoning about
distributed and concurrent systems, they restrict the language to
ensure that reduction is confluent and deterministic. ASP constitutes
the theoretical foundation for the ProActive language
\cite{CH-book}.

The Java multithreaded model of computation can be integrated with
active objects. In the Abstract Behavioral Specification Language ABS
\cite{JHSSS10} this is done by combining Creol's cooperative
scheduling model with the structuring mechanism of concurrent object
groups (cogs) \cite{ref:jcobox}.  Such a group of active objects
consists of a pool of threads each of which is generated from an
external asynchronous call and extended as a stack of internal
synchronous method calls.  Within such a pool only one thread is
executing at a time, but the granularity of interleaving is fully
controlled by the high-level release statements of the cooperative
scheduling.

Both the Encore \cite{ref:encore} and the ProActive languages
support the distinction between active and passive objects. Passive
objects are associated to (or owned by) active objects.  Since there
is no sharing of passive objects between active objects (when
communicated passive objects are copied), no data races occur, in
contrast to the multithreaded control of the cogs in ABS. Ownership
type systems have been proposed to alleviate unnecessary copying of
passive objects in this setting \cite{clarke08aplas}. While ABS is an
executable modeling language with a focus on analysis and
property-preserving code generation, both Encore and ProActive aim at
efficient runtime systems for active object languages. Whereas Encore
targets multicore platforms, ProActive has also been applied for the
distributed setting.

In parallel with this track of languages, other languages emerged, diverging more from 
the classical active-object model. 
JAC~\cite{ref:jac} stands for Java Annotations for Concurrency, it relies on a set of 
Java annotation allowing the programmer to specify what parallelism can occur inside a 
Java object with a relatively high-level perspective. It is also possible to encode some 
kinds of actors in JAC. AmbientTalk~\cite{Dedecker:2006:APA:2171327.2171349} provide 
actors with fully asynchronous futures: contrarily to other active object languages, 
futures do not provide a classical synchronisation pattern,  instead a call on a 
future triggers an asynchronous invocation that will be executed when the future is 
available.


Plus a small (or bigger) note on : 
E programming language, Erlang, 
Kilim, Akka, Go,
Panini [Reinar]
\TODO{Not sure all of them should be cited but we will decide later}




\subsection{Dimensions of Comparison between Languages}
Before presenting in details different active object languages, we  
define the key points that we consider as important regarding the design and the 
implementation of actor programming languages. 
Those dimensions of comparison, defined below, will  be used in the following to present 
the different languages.

\paragraph{Objective of the language} Identifying the objective of the language, for
which purpose it was created, is crucial to understand the design of a programming 
language.  For example, the performance of the language can be a crucial factor 
for which another aspect can be neglected, such as the accessibility of the language to 
non-experts. Or, a high expressiveness can be expected for a language targeting several 
platforms. Having a precise objective for a language generally defines the trade-offs 
that have to be taken between 
several aspects. 
%and for this reason, 
%in the following each language description 
%will start by a brief presentation of the language's objectives.

\paragraph{Degree of synchronisation} 
	The way a concurrent programming language is  designed highly 
	depends on the degree of synchronisation that can be expressed in it. Each language has a 
	different set of synchronisation primitives, although for active object languages, 
	there is a narrow set of such primitives. In some languages, inspired from pure actors, there is no synchronisation primitive: concurrent entities evolve in a completely 
	asynchronous manner and there is no particular instruction waiting for some event to happen. The 
	only synchronisation between processes is due to the causal dependency created by the 
	flow of messages. 
%	In this case, a forged synchronisation 
%	can be obtained by choosing the set of messages an actor can handle at a given point 
%	in time. 
	On the other hand, many active object languages use futures as a synchronisation 
	mechanism. 
	A future represents a
	result that has not been computed yet. When the result is computed, it can be 
	automatically available or explicitly retrieved, depending on the language design; we then say that the
	future is \emph{resolved}. A process can also block waiting for a future to be resolved.
	In active object languages, futures represent the result of asynchronous 
	invocations, and, in most of active object languages, an access to a future is a  
	synchronisation point
	between concurrent entities. Somehow such a synchronisation makes 
	programming easier as it keeps a sequential workflow that the programmer can trust.
	Several active object languages also support cooperative scheduling: a thread can be 
	suspended to handle another message, and resumed later. The suspension can be 
	triggered when checking whether a future
	is resolved or not. This breaks the sequential processing of messages and requires a 
	better concurrent expertise from the programmer, but also allows him/her to write 	
	programs that are more efficient and less deadlock-prone.

	\paragraph{Degree of transparency} Another difference between languages 
	with concurrency constructs is the number and the complexity of the programming abstractions the programmer has to 
	know and master; among them some are explicitly visible in the program and some are 
	hidden from the programmer, we say that they are transparent. For example,  some 
	active object languages feature transparent futures, which means that, on one hand
	variables which contain futures are not explicitly distinguished with a special static type, and on the other hand, that there is no 
	explicit instruction for accessing a future: an access to a future object is blocking 
	if the future is not resolved yet.
%some active object languages 
%feature transparent asynchronous method invocations, i.e. asynchronous method 
%invocations that are 
%syntactically identical to synchronous ones. S
	In general, the more transparency, the easier it is to write 	basic programs, 
	because the programmer do not need to know all the intricacies of the 
	programming model and because the parallelism can be provided automatically. However, when targeting 
	more complex applications, the benefits of transparency is weakened, as exposing 
	the programmer to all the programming abstractions that are used can make 
	programming, debugging,
	or optimization easier. 
	

	\paragraph{Degree of data sharing} Data sharing is a crucial aspect of concurrent and 
	distributed programming. Depending on the target application domain and the potential 
	for distributed execution, the constraints regarding data sharing can be vary a lot 
	and be stronger or weaker. 		
	The most efficient way to communicate data between 
	different cores of the same machine is to share the data, whether in a distributed 
	setting, copying the information to the different consumers of the data is often the most 
	efficient as it avoids additional communications for peaking the data source. 	
	Aside runtime efficiency, the complexity of the language implementation varies 	
	accordingly to the degree of data sharing. For example, copying data raises the 
	problem of 
	data consistency as data modifications may not be reflected on all copies.  
	But on the other hand, shared data access involves additional communication to the 
	shared memory, and additional synchronisation and delays. In practice, active object 
	and actor languages that have distributed runtimes often use a ``no shared memory'' 
	approach that favors 	
	data copies for objects in general, and that accepts a few entities that are globally 
	known and shared. On the contrary, when all objects are shared and accessible from 
	anywhere, the 
	advantage is that the data consistency is simpler to obtain, but 
	the communication overhead is high when programs are run in distributed 
	settings. 
	%Encore is probably the richest language from this point of view, aiming at a 
	%precise control of data sharing including entities that can explicitly be shared 
	%between different actors.

	\paragraph{Formal semantics and support} To reason on the 
	properties of a language, a formal semantics is required. Most of the 
	active object languages have a well-defined formal semantics; this is probably due to 
	the fact that  active object and actor models were designed to make concurrent 
	entities run safer. Formal semantics 
	can then be used in several ways, either to prove generic properties that makes 
	programming easier, or to prove the correctness of some 
	implementation or some optimisations, or to implement formal tools for the analysis 
	of programs. A language that has a formal semantics is overall more 
	trustworthy; this is why we  take this point 
	of comparison into account. 
	
	\paragraph{Implementation and tooling support} We believe it is important to 
	compare the active object languages relatively to the  tools they 
	provide. Indeed, the implementation of an active object language and of the 
	associated tools is also a factor to 
	take into account because it might explain some constraints related to the underlying 
	technologies that are used. Finally, the tooling support 
	around a programming language ranges from utilities to help the programmer 
	designing, writing, and 
	analysing their programs, to utilities to support the deployment and execution of these 
	programs. This point of comparison and the previous one  ensure the sustainability 
	of the programming language.

\subsection{A Focus on Some Active-object Languages}


\subsubsection{Rebeca}
\paragraph{General presentation and objective of the language} 
Rebeca (\emph{\underline{Re}active O\underline{b}j\underline{ec}ts L\underline{a}nguage}) is an actor-based modeling language designed in 2001 \cite{DBLP:conf/csicc/SirjaniMM01,DBLP:journals/fuin/SirjaniMSB04}, as an imperative interpretation of Agha's actor model,  provided with formal semantics and supported by model checking tools~\cite{DBLP:journals/jucs/SirjaniBM05,DBLP:conf/birthday/SirjaniJ11}. Rebeca is designed for modeling and verification of concurrent and distributed systems with the goal to bridge the gap between software engineers and formal methods community, by being a usable and at the same time analyzable modeling language. 

As usability has been a primary goal in designing Rebeca, the syntax is shifted to be 
Java-like,  the computation and communication model of the core language is kept simple, 
and the analysis support is the model checking. Rebeca is an actor-based language without 
shared variable between actors and with asynchronous message passing (there is no 
blocking send or receive statement).
Because of the simple structure of the language, learning Rebeca is easy. Furthermore, using model checking tools needs far less expertise comparing to deduction-based analysis techniques.
%
Semantics of Rebeca helps analyzability,  Rebeca actors are isolated and hence various abstractions, and modular and compositional verification become less problematic to apply. Any tight coupling between the building modules of a model, like shared variables, wait statements, and synchronous message calls, can make the analysis more difficult.

Rebeca offers  perfect modeling capabilities and efficient model checking tools for 
concurrent distributed,  event-based applications with pure asynchronous message passing. 
Even so, sometimes synchronization among components, or different communication models  
are vital and the modeling of some applications becomes cumbersome. Following the 
strategy of ``pay for what you need'', while core Rebeca is kept simple,   the language 
is extended in different dimensions, creating the Rebeca family, to support modeling and 
analysis of a wider variety  of application domains. 
Extended Rebeca \cite{DBLP:conf/acsd/SirjaniBMS05,DBLP:journals/jucs/SirjaniBM05}, RebecaSys \cite{DBLP:journals/tecs/RazaviBSKSS10}, Variable Rebeca \cite{DBLP:journals/jucs/SabouriK13}, 
Broadcasting Rebeca \cite{DBLP:conf/fsen/YousefiGK15},  and Wireless Rebeca are examples 
of these extensions for modeling and analysis of globally asynchronous-locally 
synchronous systems,  hardware/software co-design (or system-level design), product-line 
of actors,  message broadcasting abilities, and mobile ad-hoc actors, respectively. 
  %Timed Rebeca \cite{DBLP:journals/scp/ReynissonSACJIS14}, and Probabilistic Timed Rebeca \cite{DBLP:journals/eceasst/JafariKSH14} real-time actors, and real-time actors with probabilistic behaviors
  In these extensions, the core Java-like syntax of Rebeca is preserved and only a 
  minimal  set of statements and keywords are added to support the requirements of each 
  application domain. 
  %This way, modelers learn one modeling language; although, they are able to use different extensions of Rebeca to analyze different kinds of systems or different aspects of a system. 
  %Rebeca also tailored for the domain of hardware-software co-design in \cite{DBLP:journals/tecs/RazaviBSKSS10} to be used as the back-end model checker of SystemC designs.
  %
Timed Rebeca 
\cite{DBLP:journals/corr/abs-1108-0228,DBLP:journals/scp/ReynissonSACJIS14} 
 extends Rebeca to capture real-time behavior and is  supported by customized efficient 
 analysis tools. Finally probabilistic behavior are supported by Probabilistic Timed 
 Rebeca \cite{DBLP:journals/eceasst/JafariKSH14} that can be analyzed using back-end 
 tools.


\paragraph{Language description} 
We describe Rebeca using an example of a Media Service system, depicted in Fig.~\ref{fig:rebeca-example}. In this example, there are some clients 
%which want to watch movies. They 
that send requests for watching movies to a dispatcher and the dispatcher 
non-deterministically redirects the request to media servers. A media server starts 
streaming upon receiving a request from the dispatcher. The Rebeca model of Media Service 
consists of a number of \emph{reactive classes}, which are \texttt{MediaServer}, 
\texttt{Dispatcher}, and \texttt{Client}, each describing the type of a certain number of 
\emph{actors} (also called \emph{rebecs} in Rebeca). A reactive class declares a set of 
\emph{state variables} (line 2). The local state of each actor is defined by the values 
of its state variables and the contents of its message queue. Following the actor model, 
communication is realised by asynchronous message passing. Each actor has a set of 
\emph{knownrebecs} of known actors to which it can send messages. For 
example, an actor of type \texttt{Dispatcher} knows three actors of type 
\texttt{MediaServer} (line 8), to which it can send \texttt{requestForMovie} message 
(lines 12-14). Each reactive class of a Rebeca model may have a constructor. Constructors 
have the same name as the declaring reactive class and does not have return value (lines 
3 and 21). Their  task is initializing the actor's state variables (line 3) and putting 
initially needed messages in the queue of that actor (line 21). %A properly written 
%constructor leaves the resulting actor in a valid state. -> LUDEO REMOVED: in which 
%language this is not true?

Message servers are defined in reactive classes in a similar way that methods are written in Java.
%Reactive classes declare the messages to which they can respond to, using Java-like syntax of defining methods. 
In contrast to the Java methods, \emph{message servers} of Rebeca does not have return value and their declarations start with keyword \texttt{msgsrv}. 
%
A message server is executed upon receiving the  corresponding message,  
and may include assignment statements,  conditional statements (lines 11 to 15), and  
message sending (lines 4 and 22).
Loops and periodic behavior is modeled by sending messages to itself (line 23). Since the communication is asynchronous, each actor has a \emph{message queue} from which it takes the next message to serve. The ordering of the messages in the message queues is first-in first-out.
%based on the arrival times of messages. 
An actor takes the first message from its message queue, executes its corresponding message server in  a non-preemptive way, and then takes the next message. The actor stays idle only if there is no other message in the queue.
%, and continues in.(or waits for the next message to arrive) and so on. 
A message server may include a \emph{nondeterministic assignment} statement which is used to model the nondeterminism in the behavior of a message server (line 10).

Finally, the \texttt{main} block instantiates the 
actors of the model. In the Media Service model, seven actors are created, receiving 
their known rebecs and the parameters' values of their constructors (lines 27-29).


\lstdefinelanguage{rebeca}{
  morekeywords={reactiveclass, knownrebecs, statevars, main, msgsrv, main, define, LTL, CTL, boolean, int, shortint, byte, if, else, while, for, wait, msg, reset, set, self, false, true, now, after, delay, deadline, initial},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}

\lstset{frame=tb,
  language=rebeca,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  keywordstyle=\color{blue},
  numbers=left,
  numberstyle=\color{black},
  numbersep=5pt,
  xleftmargin=5pt,
  stepnumber=1,
  frame=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2,
}

\begin{figure}
\begin{lstlisting}[language=rebeca]
reactiveclass MediaServer (5) {
	statevars {	byte maximumClient; }
	MediaServer(byte maxC) { maximumClient maxC; }
	msgsrv requestForMovie(Client client) { client.receiveMovie(); }
}

reactiveclass Dispatcher (5) {
	knownrebecs { MediaServer m1, m2, m3; }
	msgsrv requestForMovie(Client client) {
		byte selectedServer = ?(1, 2, 3);
		switch (selectedServer) {
			case 1: m1.requestForMovie(client); break;
			case 2: m2.requestForMovie(client); break;
			case 3: m3.requestForMovie(client); break;
		}
	}
}

reactiveclass Client(2) {
	knownrebecs { Dispatcher dispatcher; }
	Client() { self.watchMovie(); }
	msgsrv watchMovie() { dispatcher.requestForMovie(self); }
	msgsrv receiveMovie() { self.watchMovie(); }
}

main {
	MediaServer server1():(2), server2():(3), server3():(2);
	Dispatcher dispatcher(server1, server2, server3):();
	Client client1(dispatcher):(), client2(dispatcher):(), client3(dispatcher):();
}
\end{lstlisting}  
\caption{A simple Rebeca model}
\label{fig:rebeca-example}
\end{figure}

\paragraph{Degree of synchronisation} 
Conforming the Agha's actor model, the only communication mechanism among actors of 
Rebeca is asynchronous message passing. So, there is no synchronization among actors of 
Rebeca. 
% Ludo: removed : I am not sure this is really related to synchronisation
%The only condition that you may consider as a deadlock, is when all the message queues 
%of all actors are empty. 
% In this case, all the actors will stay idle and having progress is impossible. The 
%analysis tool sets of  Rebeca family support automatic deadlock-freedom check for the 
%models.  
In Rebeca, arrival of a message to its receiver message queue is guaranteed and the order 
of the messages sent by one actor to the same actor is preserved creating some form of 
guaranteed order of execution.

%Marjan
% rebeca extensions
In  Extended Rebeca \cite{DBLP:conf/acsd/SirjaniBMS05,DBLP:journals/jucs/SirjaniBM05}, Rebeca is enriched with a concept of component as a collection of rebecs that are more tightly coupled, rebecs inside a component can be synchronized by a handshaking communication mechanism. 
%At the highest level of abstraction, components only interact asynchronously via broadcasting anonymous messages. At a lower level of abstraction (within a component), computations, on the one hand are driven by asynchronous messages, and on the other hand can be synchronized by a handshaking communication mechanism. 

%RebecaSys
\LUDO{I do not really understand this paragraph and hoew this is related to 
synchronisation}
In RebecaSys, the syntax is extended with \textit{wait} statements, and a block of 
\textit{global variables} at the beginning of the model.  RebecaSys is defined as a 
back-end language,  SystemC codes are automatically mapped to RebecaSys and then are 
formally verified. As a RebecaSys model is only created from a SystemC model 
automatically, a controlled way of using global variables  is enforced, so, racing is 
avoided. Hardware/Software co-design languages support a cycle-based synchronization 
among all parallel processes. For example, if more than one module or process can write 
in a SystemC variable then two global variables in RebecaSys are defined, for pre- and 
post- values for each cycle.  
A wait statement is also added, where a rebec waits until a Boolean expression becomes true. 
%This way we could keep the same model checking tools and techniques (including reduction techniques)
% ... multiple writer will be two variable, for pre-post, 
%Wait until a boolean expression becomes true.
%
%Variable Rebeca: 


In Broadcasting Rebeca,
broadcast statement is added to Rebeca, but there is no known rebecs, so direct message 
sending to an actor is impossible.
% there is no shared variable or synchronization
 Wireless Rebeca is an extension of Broadcasting Rebeca where in addition to broadcast to all, there are possibilities of multicast to neighbors and unicast to sender.
Neighbours of an actor are actors which sent messages to it. Here, unlike in Rebeca, we have possibility of a message getting lost, as a neighbor may move out of the reach of an actor.
\LUDO{what is your message here concerning synchronisation?}

\paragraph{Degree of transparency}
In Rebeca, programmers are not exposed to the mechanism of asynchronous communication among actors. 
The  syntax  is the same as the syntax for sequential programming. 
%of Java, as a sequential programming language. 
There is no way to access the contents of message queues, and acknowledgments are not sent upon starting or finishing the execution of a message server. In addition, programmers does not have any control on the interleaved execution of actors; i.e., the internal thread of an actor takes a message from the actor's message queue and serve it in an isolated environment, regardless of the states of other actors.
%

In Rebeca, the modeler cannot peek into the  message queues and have information about a message without taking the message. 
In modeling various applications, it is observed that sometimes a pattern-matching mechanism for choosing a message to take from the queue, like what is provided by Erlang,  could be very handy.
\LUDO{is this more related to transparency or to synchronisation (on another message)?}

This transparency holds for all the extensions of Rebeca.
In Timed Rebeca and Probabilistic Timed Rebeca, the message queue is changed into a message bag with time-stamped messages, nevertheless, the transparency is not changed.

\LUDO{I would remove this paragraph, at least from here and perhaps globally: I feel like 
it is not related to the main subject ... I think}
Variable Rebeca is defined for designing product line software, and allows all the 
constructs of a model (i.e. reactive classes, state variables, message servers, 
statements, ...) to be annotated. This way the modeler can show which constructs are 
associated to each  product, and hence different concrete models are built based on the 
annotated model. 

\paragraph{Degree of data sharing}
%Following the actor's definition, 
There is no shared variable among actors in Rebeca. Parameters in sending messages among actors are passed by value.
%, so, the value of parameters are sent to the other actors not their references. 
This  is also valid when a reference to an actor is sent as a parameter. In this case, the sent reference is created by shallow-copy of the original reference, i.e., avoiding creation of a new actor because of message passing (e.g. Fig~\ref{fig:rebeca-example} line 9) .
%Marjan: compare with ASP?
% GALS Extended Rebeca
% RebecaSys
%None of the languages in Rebeca family allows shared variables.
In RebecaSys, a block of global variables can be defined at the beginning of the model, but these variables are used in a controlled way as explained above.
\TODO{Ludo: check again when I understand better the previous section on rebecasys}

\paragraph{Formal semantics and support}
To support analyzability and develop formal verification tools, Rebeca and its extensions are all equipped with formal semantics.
%One the main goal of Rebeca family languages is formal modeling and verification of actor-based models. So, all of the extensions of Rebeca are equipped with formal semantics. 
Formal semantics of Rebeca is presented in \cite{DBLP:journals/fuin/SirjaniMSB04}, and  the model checking toolset of Rebeca is developed based on this semantics. 
%Later, it is presented in Algebra of Communicating Processes (ACP) for better support of reduction techniques \cite{DBLP:conf/acsd/HojjatSMG07}. 
An ACP (Algebra of Communicating Processes) semantics is also presented in \cite{DBLP:conf/acsd/HojjatSMG07}.
%
% Marjan: if we mention BRebeca we have to mention others too
%Semantics of broadcasting Rebeca is presented as SOS (Structural Operational Semantics) rules in \cite{DBLP:conf/fsen/YousefiGK15} to provide a solid basis for its model checking toolset. 
%
Semantics of Timed Rebeca is presented as timed automata \cite{DBLP:journals/scp/KhamespanahSSKI15}, timed transition system \cite{DBLP:conf/facs2/KhamespanahSVK15}, real-time Maude \cite{DBLP:journals/scp/Sabahi-KavianiK15}, and floating time transition system \cite{DBLP:conf/facs2/KhamespanahSVK15}.
Floating time transition system is an innovative semantics for real-time actors, that results in a significant amount of reduction in time and memory consumption while verifying timed actor models. 
%It is also presented in real-time Maude to support the analysis of dynamically creating real-time actors \cite{DBLP:journals/scp/Sabahi-KavianiK15}. 
Formal semantics of Probabilistic Timed Rebeca is presented as TMDP in \cite{DBLP:journals/eceasst/JafariKSH14}. 


\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\begin{table}
\tbl{Summary of comparing different members of Rebeca family language\label{tab::Rebecacomparison}}{%

\scriptsize
    \begin{tabular}{|L{1.8cm}|L{1.7cm}|L{1.3cm}|L{1.9cm}|L{1.3cm}|L{1.6cm}|L{1.2cm}|}
    \hline
    & \textbf{Objective of the language} & \textbf{Degree of Synchronization} & \textbf{Degree of Transparency} & \textbf{Degree of data sharing} & \textbf{Formal Semantics and Support} & \textbf{Tooling Support} \\
    \hline
    \textbf{Rebeca (R)} & modeling and verification of actors & NO & message queues and  interleaving of executions  & NO & SOS, ACP & YES \\
    \hline
    \textbf{Extended Rebeca (ER)} &  Globally asynchronous-locally synchronous Systems & local synchronous messages & (R) & NO & SOS & NO \\
    \hline
    \textbf{RebecaSys (RS)} & Hardware / Software co-design & wait statement & (R) + synchronization over global variables & Global variables & LTS & YES \\
    \hline
    \textbf{Variable Rebeca (VR)} & Modeling and analysis of product-lines of actors & NO & (R) + features inclusion regarding to the selected product & NO & SOS & Needs helps of an expert\\
    \hline
    \textbf{Broadcasting Rebeca (BR)} & Modeling of actors with broadcasting abilities & NO & (R) + broadcasting to all actors mechanism & NO & SOS & YES \\
    \hline
    \textbf{Wireless Rebeca (WR)} & Modeling of ad-hoc mobile networks & NO & (BR) + handling the connectivity of nodes + multi-casting & NO & SOS & YES \\
    \hline
    \textbf{Timed Rebeca (TR)} & Realtime actors & delay statement & (R) + Progress of time  & NO & SOS, RT Maude, Timed Automata, FTTS & YES \\
    \hline
    \textbf{Probabilistic Timed Rebeca (PTR)} & Probabilistic readtime actors & (TR) & (TR) & NO & SOS & Backend (IMCA \& Prism) \\
    \hline
    \end{tabular}
}
\end{table}

\paragraph{Implementation and tooling support}
There is a wide range of analysis and mapping toolsets for Rebeca family models which are 
accessible from the Rebeca home page, see \url{http://rebeca-lang.org}. The majority of 
these toolsets and libraries are integrated in Afra, the modeling and model checking IDE 
of Rebeca family models. 
Rebeca has been taught in different courses at different universities, including 
Reykjavik University, Sharif University, University of Tehran, and University of Illinois 
at Urbana-Campaign. Afra is an \textsc{Eclipse} plug-in that is used by students of these 
classes, and provides the following facilities: \LUDO{do we really want to all list the 
universities where each language is taught???}
%, Afra is an integrated tool which has been used in teaching classes different universities (Tehran, sharif, RU, UIUC)visualize and step through the counter example - queue overflow - . 
\begin{itemize}
\item Eclipse-based editor with syntax highlighting for writing the models and  properties
\item Compiler and build system integrated with error location mechanism of Eclipse
\item Model checking tool for  LTL and CTL model checking of Rebeca,
and  Floating Time Transition System generator and analyzer for Timed Rebeca
\item State space generators, compatible with CADP and $\mu$CRL analysis and 
visualization toolsets
\item Facilities for traversing counter examples in case of property violation
\end{itemize}

Additionally, there are other stand-alone tool-sets for the analysis of Rebeca  
models:
\begin{itemize}
\item Simulator backend tool for Rebeca and Timed Rebeca models, generating  Erlang 
codes   \cite{DBLP:journals/scp/ReynissonSACJIS14}

\item Analysis backend tool for Timed Rebeca, generating Real-time Maude models \cite{DBLP:journals/scp/Sabahi-KavianiK15}

\item Model checking tool chain for  analysis of Probabilistic Timed Rebeca \cite{DBLP:journals/eceasst/JafariKSH14} using backend tools PRISM and IMCA

\item SysFier: model checking tool for SystemC designs  \cite{DBLP:journals/tecs/RazaviBSKSS10}

\item Sarir:  $\mu$CRL2 backend model checking tool for Rebeca \cite{DBLP:conf/acsd/HojjatSMG07}

\item Distributed model checking tool for Rebeca models \cite{DBLP:journals/eceasst/KhamespanahSMSR15}

\item Model checking tool for Broadcasting Rebeca \cite{DBLP:conf/fsen/YousefiGK15}

\item Bounded Rational LTL model checker of Rebeca  \cite{DBLP:conf/fsen/BehjatiSA09}

\item Guided search engine for deadlock detection in Rebeca models \cite{DBLP:conf/facs2/SigurdarsonSBR12}

\end{itemize}


For the summary of the mentioned aspects and comparing the features the members of Rebeca 
family refer to Table~\ref{tab::Rebecacomparison}.
\subsubsection{ABS}

\lstset{ morekeywords={module,export,import, from, interface, class,
    implements, await, get, new, local,release, suspend} }

\paragraph{General presentation and objective of the language} ABS (\emph{A}bstract
\emph{B}ehavioral \emph{S}pecification) \cite{JHSSS10} is an
object-oriented, concurrent modeling language developed since 2009 in
a series of EC-funded research
projects.\footnote{\url{www.hats-project.eu},
  \url{www.envisage-project.eu}} Its ancestors include \textsc{Creol}
\cite{Elinar2006} and \textsc{JCoBox} \cite{SchaeferP10b}.

In contrast to design-oriented or architectural languages, ABS code is
fully executable. There is a simulator as well as several code
generation backends (at the moment, for Java, Haskell, and Erlang. At
the same time, ABS abstracts away from features that make automatic
analysis difficult in mainstream programming languages. For example,
ABS has an object model, but it does not support code inheritance and
it enforces programming to interfaces as well as strong
encapsulation. It retains, however, modeling features that are
essential in realistic applications, for example, aliasing and
unbounded object creation.

The main design goal of ABS was to create a language that permits to
specify complex behavior of concurrent objects in a concise, natural
manner, while keeping automated analysis of that behavior feasible and
scalable. 

There are extensions of ABS to model product variability
\cite{HHJLSSW12} as well as time and other resources
\cite{johnsen15jlamp}, but these are considered to be out of scope of
the present article.

\begin{figure}
  \centering
\begin{tikzpicture}[scale=1]\small
\node[rectangle, 
          ,shading=axis,shading angle=180,top color=white,bottom color=black!25, line width=2pt,
          minimum width=9cm, minimum height=.6cm, draw] (ass) at (0,4.5) 
          {History-Based Behavioral Interface Specification};

\node[rectangle, 
          ,shading=axis,shading angle=180,top color=white,bottom color=brown!20, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (mod)
          at (0,3.9) {Syntactic Modules}; 

\node[rectangle, 
          ,shading=axis,shading angle=180,top color=white,bottom color=blue!40, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (async)
          at (0,3.3) {Asynchronous Actor-Based Communication}; 

\node[rectangle, 
          shading=axis,shading angle=180,top color=white,bottom color=blue!20, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (cog)
          at (0,2.7) {Concurrent Object Groups (COGs)}; 

\node[rectangle,
          shading=axis,shading angle=180,top color=white,bottom color=green!40, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (imp) at
          (0,2.1){Imperative Language};

\node[rectangle,
          shading=axis,shading angle=180,top color=white,bottom color=green!20, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (object) at
          (0,1.5){Object Model};
\node[rectangle, 
          shading=axis,shading angle=180,top color=white,bottom color=yellow!60, line width=2pt,
          minimum width=9cm, 
          minimum height=.6cm, draw] (exp) at (0,0.9) {Pure
            Functional Programs};

\node[rectangle
          ,shading=axis,shading angle=180,top color=white,bottom color=yellow!40, line width=2pt,
          minimum width=9cm, minimum height=0.6cm, draw] (adt) at
          (0,0.3){Algebraic (Parametric) Data Types};
\end{tikzpicture}
\caption{Syntax layers of the ABS language}
\label{fig:layer}
\end{figure}

\paragraph{Language description} 
In Fig.~\ref{fig:layer} the language layers of ABS are
displayed. Based on parametric (first-order) abstract data types, a
simple, pure, first-order functional language with pattern matching
and strict evaluation is defined. On top of that are objects and a
standard imperative layer. So far, this is very much like
\textsc{Scala}-light.

The central issue for achieving automated, scalable analysis is the
design of the concurrency model. Formal analysis of multi-threaded
languages with interleaving semantics, for example, \textsc{Java},
while principally possible \cite{BlomHuisman14}, is highly complex and
seems to be out of scope at the moment for relaxed memory consistency
models. To achieve feasibility, the concurrency model of ABS carefully
restricts the possible interactions among concurrent tasks while still
allowing to describe complex, realistic behavior of asynchronous
systems.  
%
More precisely, the ABS concurrency model is based on cooperative
scheduling. This means that no task is preempted (interrupted) unless
its modeler explicitly allows to do that. There are two expressions in
ABS that explicitly release control: \lstinline{release} and
\lstinline{await}. The former is unconditional while the latter has a
Boolean argument and can be used to synchronize with another task.

\begin{figure}
  \centering
\begin{lstlisting}[numbers=left,xleftmargin=4ex,escapechar=\%]
module Services;
import Data, init, modify from CustomerData;

interface Server {
  Unit process(Fut<Data> fd);
}

class Service implements Server {
  Unit process(Fut<Data> fd) {
    await fd?; %\label{abs:process:awaitd}%
    Data rd = fd.get;
    rd.modify(); %\label{abs:process:sync}%
  }
}
{ // main block
Server s = new local Service(); %\label{abs:main:begin}%
Data d = new local Data(); 
Fut<Data> fd = d!init(); %\label{abs:main:init}%
s!process(fd); %\label{abs:main:end}%
}
\end{lstlisting}  
  \caption{A simple ABS model}
  \label{fig:abs-simple}
\end{figure}

%% Besser producer/consumer?

% module Services;

% interface Server {
%   Unit produce();
%   Unit consume();
% }

% class Service implements Server {
%   Int MAX = 17;
%   Int stock = 0;
  
%   Unit produce() {
%     while (True) {
% 	  await stock < MAX;
%       stock = stock + 1;
%       }
%   }
  
%   Unit consume() {
%     while (True) {
%       await stock > 0;
%       stock = stock - 1;
%       }
%   }
% }

% { // main block
%   Service s = new Server();
%   s!produce();
%   s!consume();
% }

We explain the concurrency model of ABS with help of the code in
Fig.~\ref{fig:abs-simple}. The unit of distribution in ABS is a
\emph{concurrent object group} (COG) which can be thought of as a set
of tasks that share a common heap and a single processor. Each task
executes code owned by an object and at most one task is active in a
given COG at any time. New tasks are created by asynchronous method
calls as well as, initially, by selecting the main block of a
module. An example of the latter is the code in
lines~\ref{abs:main:begin}--\ref{abs:main:end}.

Line~\ref{abs:main:begin} declares and creates a new object with
interface type \lstinline{Server} using the implementation in class
\lstinline{Service}.  The directive \lstinline{local} places the
object in the current COG. Without \lstinline{local} a new COG is
created together with the object. The next line declares and creates a
data object (note that interface \lstinline{Data} must be imported) in
the same COG. Hence, \lstinline{s} and \lstinline{d} share the same
heap. Line~\ref{abs:main:init} calls an initialization method
(implementation not shown) on the data. The notation ``\lstinline{!}''
signifies an asynchronous call. Its effect is to create a new task in
the COG of \lstinline{d} that executes the code of
\lstinline{init()}. Asynchronous calls do not interrupt the caller, so
the statement following the call is immediately executed. Therefore,
we need a handle by which to retrieve the result of an asynchronous
call once its result has been computed. In ABS the type of such
handles have a future annotation. As can be seen in
Line~\ref{abs:main:end}, it is possible to pass futures as
parameters. This makes it possible to use the result of an
asynchronous call in different places without copying it.

After execution of the main block finished, two tasks in the current
COG are waiting, corresponding to the calls to \lstinline{init()} and
\lstinline{process()}, respectively. None of them could have started
while the main block was still executing, because there was no
synchronization expression in the latter. ABS does not determine which
of \lstinline{init()} and \lstinline{process()} is started first. In
fact, ABS can be parameterized with different scheduling
strategies. The static analyzers of ABS take all possible scheduling
sequences into account.

A first synchronisation point is reached in
Line~\ref{abs:process:awaitd} of \lstinline{process()}. It ensures
that the value of the future \lstinline{fd} is available. If
\lstinline{init()} had not been scheduled before, it will be now. Once
the value of \lstinline{fd} is available, it is retrieved with a
\lstinline{get} expression. Note that \lstinline{rd} and \lstinline{d}
might well be aliased. The standard ABS idiom for asynchronous calls
in ABS is as follows:

\begin{center}
  \lstinline{Fut<T> fx = o!m(); ... ; await fx?; T x = fx.get;}
\end{center}

In many cases \lstinline{await} and \lstinline{get} follow an
asynchronous call directly. For this common situation the abbreviation

\begin{center}
  \lstinline{T x = await o!m();}
\end{center}

is provided which avoids the declaration of an explicit future.

Line~\ref{abs:process:sync} contains a synchronous call which is also
possible in ABS. It results in sequential behavior, i.e., yields the
processor to the callee and blocks the caller until it
returns. Obviously, here we are interested only in the side effect
that \lstinline{modify()} has. Synchronous calls are only permitted in
the COG of the caller object. \LUDO{add a sentence for release/suspend} 
\KIKO{In Encore, await returns void and in ABS, it seems to return
a type T bound to o!m(). Maybe this should be explicitly said. At first, I assumed
that await fx? returns void}

\paragraph{Degree of synchronisation} 
If one attemps to retrieve a future value that is not yet ready, this
results in blockage of its COG until the value becomes
available. Obviously, this can easily lead to deadlocks. Many
deadlocks can be avoided by guarding \lstinline{get} expressions with
an \lstinline{await} (Line~\ref{abs:process:awaitd}). Clearly, not all
deadlocks can be prevented in this manner. In practice, however,
deadlocks are easy to avoid, because synchronisation points are
explicit. In addition, ABS comes with an automated deadlock analysis
tool for ABS \cite{CGM:SoSym2014} that detects all potential
deadlocks.  An important point is that between explicit
synchronisation points (\lstinline{release}, \lstinline{await}) no
data races can occur and computations are, therefore, deterministic.

\LUDO{while I agree concerning data races, the order of service of requests is also non 
deterministic, thus return also creates non-determinism somehow}

\paragraph{Degree of transparency}

ABS is an abstract language and implementation-specific aspects
including scheduling, message queueing, object representation are
hidden from the modeler. Abstract data types and interfaces can be
used to postpone detailed design decisions while still permitting
analysis of those aspects of a model that are fully specified.

\LUDO{added the following paragraph}

In ABS the user controls however the potential interleavings of the tasks and explicitly 
writes task release points using \lstinline{release} and \lstinline{await}. As a 
consequence he is exposed to the notion of asynchronous vs.synchronous calls, futures, 
and thread interleaving.

\paragraph{Degree of data sharing}
\LUDO{I do not understantd this first part. I thought all AOs were accessible from any 
COG and all other data was immutable and thus safe; can you explain better}
Between different COGs no data sharing is possible. The attempt to
call a method in a remote COG with a local class argument results in a
runtime error. Within the same COG all tasks have acess to a common
heap and can modify shared data. 
\LUDO{the rest is OK but I would mention that all objects of the COG are accessible (via 
nmethod calls) if this is indeed true}
However, objects have strictly
private visibility, that is, they can only directly access their own
fields. The fields of any other objects must be accessed by explicit
method calls (getter/setter methods).

\paragraph{Formal semantics and support}

ABS has a fully formal, small step operational semantics
\cite{hats-d1.2} that permits formal soundness theorems for the
various analyses that are available for ABS. This semantics is
directly expressed in terms of rewrite rules in \textsc{Maude} format
\cite{ClavelDELMMT07} which yields a sound interpreter for ABS.

In addition there is an axiomatic semantics in the form of a program
logic~\cite{ref:key}. The behavior of interfaces and classes can be
specified by invariants over the histories of symbolic states as contracts 
between processes. 
Because preemption is excluded in ABS it is sufficient to establish
invariants upon object creations, at explicit synchronisation points and 
upon the termination of methods.  It is possible to prove a composition 
theorem for ABS about the relation between local and global invariants
\cite{Din14fac}. This makes it possible to prove global behavioral
properties of an ABS model by (method-)local reasoning.

\paragraph{Implementation and tooling support}

As ABS has been developed with the goal of being analysable, there is
a wide range of tools available. Most of them support the full ABS
language and are fully automatic. An overview of several of the tools
is available as \cite{BFH14,WongAMPSS12}.

There is an \textsc{Eclipse} plug-in that provides a development
environment for ABS and integrates most tools, see
\url{http://tools.hats-project.eu/eclipseplugin/installation.html}. An
alternative is the web-based ABS collaboratory at
\url{http://ei.abs-models.org:8082/clients/web/} which requires no
installation and also permits to try out most ABS tools.  Here is a
list of currently supported tools for ABS:

\begin{itemize}
\item editor with syntax highlighting and integrated build system,
  including compiler error location;
\item simulator/interpreter with interactive debugger;
\item visualization of ABS model execution as sequence diagram;
\item code generator backends for Erlang, Haskell, ProActive~\cite{rochas:hal-01065072}Java
  8~\cite{serbanescuNABN14a};
\item a glass box test case generator for \emph{concurrent} models \cite{AlbertAGM15};
\item a sound deadlock analysis \cite{CGM:SoSym2014};
\item a worst-case resource analysis that can be parameterized with a
  cost model for execution time, memory consumption, transmission
  size, peak cost and various other cost categories \cite{AlbertAFGGMPR14};
\item a deductive verification tool for expressive, history-based
  property specifications \cite{ref:key}
\end{itemize}


\subsubsection{ProActive and ASP}
\paragraph{General presentation and objective of the language}
ASP~\cite{ref:asp} is an active object programming language especially designed for
programming and deploying distributed systems. ProActive is a Java library implementing the semantics
of the ASP calculus. The language is designed taking the constraints of distributed
programming into account, and relies on Remote Method Invocation as the communication layer even though
other communication mechanisms can be used. ProActive is intended for distributed execution; it
is a middleware that supports application deployment on distributed infrastructures such
as clusters, grids and clouds. Several choices for the design of the language can be
explained by these practical concerns.

One of the design choices for ASP and ProActive is to ensure maximal transparency for the 
programmer: active objects and futures are manipulated like usual Java objects. The 
ProActive middleware automatically triggers asynchronous remote invocations and performs automatically blocking synchronisation when 
needed. ProActive is thus intended to Java programmers that are not particularly experts 
in concurrent and distributed programming. 

Another choice is that active objects are coarse grain entities. We create a dedicated 
thread for each of them and they come with a quite heavy machinery  for managing each 
object and communicating between objects. The machinery ensures all the distributed 
nature of the computation. As a consequence, using the ProActive library, 
it is not possible to instantiate thousands of active objects on the same core. Of course 
the number of passive objects is not particularly restricted.

Since 2010, ASP features
\emph{multi-active objects}~\cite{HHI2013:mao} meaning that in each active object, several
threads can run in parallel and process several requests of this active object, but each thread is still isolated inside a single activity. 
Such multi-active objects feature at the same time local concurrency and global 
parallelism.

\paragraph{Language description}
In ASP, active objects coexist with objects that are not active; we call them 
\emph{passive} objects. An active object together with its 
service thread(s) its passive objects, and its request queue is called an activity. 
Each passive object is placed under the responsibility of an active object.
Only active objects are accessible between activities. The 
objects that are not active are only accessible within an activity; if those 
objects need to be used by several activities, they are copied in each activity. Based on this 
clear separation, the activity is the unit of distribution, which matches the usage of one memory space per activity. 
In ASP, when using multi-active objects, 
several threads can execute in the same activity, thus several threads can potentially access the objects of an activity. 

The language is transparent: method calls are automatically turned into asynchronous 
requests if the targeted object is a remote active object, otherwise it is a synchronous, local method call. Similarly, futures are 
implicitly created upon asynchronous calls. Futures are also transparently manipulated: 
a wait-by-necessity is automatically triggered upon an access to an unresolved future. 
In ASP, futures are first-class: they can be passed between activities. In this case, 
when the future is resolved, the result is automatically updated at all locations.

ProActive offers an API to create active objects, 
and a runtime for handling ASP features. The following is an example of ProActive program:
\lstset{
	emph={parameters,node}, 
	emphstyle=\itshape
} 
\begin{lstlisting}
O o = PAActiveObject.newActive(O.class, parameters, node);
T t = PAActiveObject.newActive(T.class, parameters, node);
V v = t.bar(); 	// implicit asynchronous method call
o.foo(v); 	// v can be passed without blocking
v.foobar(); 	// potential wait-by-necessity on v
\end{lstlisting}
%%% THE FOLLOWING MIGHT BE TOO DETAILLED %%%
 An active object is created using \code{newActive}, instead of the \code{new} of Java.
 The \code{newActive} primitive takes as parameters the class to instantiate, the parameters of the
 constructor, and the node on which the active object will be deployed.
 The variable \code{v} is the result of an asynchronous call; it is an
 implicit future.
% The dynamic type of \code{v} is a future that is a dynamically created subtype of 
%\code{V}. 
 When the future value is needed to continue execution, such as in \code{v.foobar()}, 
 a wait-by necessity automatically occurs if the future is not resolved. In ProActive,  
 proxies are used to handle transparently active objects and futures.
% In ProActive, when an active object is created, it is registered in the
% RMI registry delivered with Java. A local reference to this active object is also created: 
% a proxy that delegates invocations to the active object.

 The principle of the multi-active object programming model is to execute multiple
 requests of an active object in parallel, while controlling the concurrency. 
 In practice, the 
 programmer can declare which requests (i.e. which methods of the active object) can be 
 safely executed in parallel. Such requests are 
  called \emph{compatible} requests. The internal scheduler of an active object will allocate by default
 as many threads as necessary to run compatible requests in parallel.
  In 
 ProActive, multi-active object features
  can be used through a metalanguage, based on Java annotations. The 
 following is an
 example of multiactive object annotations in ProActive: 
 \lstset{morekeywords={@DefineGroups,@Compatible,@DefineRules,@Group,@MemberOf} }
\begin{lstlisting}
@Group(name="group1", selfCompatible=true)
@Group(name="group2", selfCompatible=false)
@Compatible({"group1", "group2"})
public class MyClass {
  ...
  @MemberOf("group1")   
  public ... method1(...) { ... }
  
  @MemberOf("group2")   
  public ... method2(...) { ... }
}
\end{lstlisting}
In this example, two groups of requests are defined, each of them holding one method. 
The two groups are declared to be compatible (so as their method, by extension). The 
\code{selfCompatible} parameter defines whether two different requests of the same group 
are allowed to run in parallel. At runtime, a ready request is automatically executed if 
it is compatible with requests that are already executing and with older requests in the 
queue (to avoid starvation). 

Without annotations, a multi-active object is a mono-threaded active object, 
without 
any local parallelism nor any possible race condition. Programming a mono-threaded active object-based application with ProActive is thus extremely simple. If some 
parallelism is desired, a compatibility should be declared between requests 
that can be safely interleaved and for which execution order do not matter.
To define a compatibility between two requests, the programmer can also use runtime information such as the request parameters or the object's 
state. Programming a multi-active object-based application with ProActive is thus a bit more difficult than programming mono-threaded ProActive active objects, but it is less complicated than programming with raw threads and low-level synchronization mechanisms while giving a competing parallelism compared to those ancient techniques.
However, if even more parallelism, out of the scope of request compatibility, is required, the programmer can still define more requests as compatible, and prevent unexpected behaviours with traditional low-level Java synchronisation 
primitives, this mixed approach goes beyond the traditional 
active-object model.

Other high-level specifications are available in multiactive
objects~\cite{henrio:hal-00916293}, such as request priority. To avoid thread explosion,
it is also possible to set a limit on the number of threads running in parallel.  The
limit can be applied in two ways: a hard limit restrains the overall number of threads
whereas a soft limit only counts threads that are not in wait-by-necessity. 
Additionally, threads can be limited per group.

As a conclusion, ASP and ProActive are based on the multiactive object programming
model.
This model is well adapted to non-expert because it provides high-level features for distribution 
and safe concurrency.

\paragraph{Degree of synchronisation}
In ASP, the only blocking synchronisation is the wait-by-necessity on a future. In this case, as requests run until completion, potential deadlocks can arise in 
case of reentrant calls, especially if no compatibility annotation is specified. 
However, synchronisation only occur when the future value is actually needed and, in 
particular, future references can safely be transmitted between activities without 
requiring any additional synchronisation, which limits blocking synchronisation leading 
to deadlocks.
Deadlocks can also be removed by using multi-active objects especially with no limit or a 
soft limit of the number of threads. In particular,
when a thread becomes in wait-by-necessity, it is not counted in the soft thread limit of 
the active object any more, so the wait-by-necessity event potentially causes the start 
of another request.

\paragraph{Degree of transparency}
In ASP the programmer is not explicitly exposed at all to the notion of future and in a moderate way to the notion of active object (at active object creation time only). The syntax is the same as the syntax for sequential 
programming, there is no specific construct for waiting a future value or performing an 
asynchronous call. Very often a sequential code can be reused unchanged in a distributed 
setting.

When dealing with multiactive objects, the programmer is exposed to the notion of 
parallel treatment of requests, the programming model becomes more explicit concerning 
concurrency aspects. 

\paragraph{Degree of data sharing}
ASP follows a strict policy of absence of sharing between active objects. Objects that are 
not active objects and are passed by copy between activities (as request parameters or request 
results). Of course this mechanism applies also to objects that referenced by passed 
objects; a deep-copy mechanism is used ensuring that, when objects are transmitted 
between activities, they are copied as well as all their dependency on the destination 
side. This mechanism, which is the one used by RMI, slows down request invocation in practice because 
of the time spent to transmit data, but accelerates request treatment because there is no 
need to contact another activity to get the value of the request parameters. Note that 
active object and future references are passed by reference.

There is no coherency ensured between the different copies of a passive object, thus if 
the user wants to ensure that an object has a unique coherent state, he should not 
transmit it by copy, and in this case transmitting an active-object reference instead would be the best choice.

\paragraph{Formal semantics and support}

Several papers formalise the semantics of ASP. The first of them formalised the 
mono-threaded part of the language and proved some determinacy 
properties~\cite{CHS:POPL04}. In particular, this paper proved that the order of future 
updates has no influence on the execution and that the only source of non-determinacy in 
mono-threaded ASP is when two activities can send a request to the same destination.
A functional fragment of the calculus has also been 
formalised in Isabelle/HOL~\cite{HKL:SCP11}. A specific semantics has been exhibited in 
order to evaluate a functional ASP program without risk of having a deadlock; the absence 
of deadlock has been proved in Isabelle/HOL.
The full semantics of imperative ASP with multi-threaded activities is published 
in~\cite{HHI2013:mao}.


\paragraph{Implementation and tooling support}
ProActive is the Java library implementing ASP semantics. 
In ProActive, in order to transparently handle active objects and futures, a proxy is created for each of them. Proxies encapsulate all needed code to perform asynchronous, remote method invocations and wait-by-necessity behaviour. Whenever an active object or a future is given as parameter of a call to an active object,  it is in fact their proxy that is copied. This way, all copies of a proxy of an active object/future points to this same active object/future. 
Another particular aspect of ProActive deals with
the deployment of ASP active objects on distributed infrastructures. The design choices of the programming language typically target a high performance of distributed ProActive applications. 
In order to settle active objects on distributed infrastructures, ProActive features a deployment specification mechanism. The goal of this mechanism is to make the physical deployment independent from the the deployment logic. This is possible by having a binding from virtual node names, used in the source code, pointing to machine addresses or names. In practice, this binding is implemented in XML configuration files. Also, since the binding is made at \emph{deployment time}, changing infrastructure for a given ProActive application is only localised in a few files and does not require application recompilation. 
Several machines can be aggregated under a single virtual node name in the deployment logic, for example to provide the virtual node with some properties or non functional deployment options (such as fault tolerance or logging).


As shown in~\cite{BHR2014:gcm}, ProActive active objects, and active objects in general,
provide a convenient programming model for component based composition of distributed
applications. The ProActive library implements the GCM distributed component model. In
this context, the Vercors
platform~\cite{HKM-FASE16} provides
verification capacities for ProActive components; Vercors consists of an Eclipse plugin
for designing component systems; from this point the Vercors platform can on one hand
verify the correct behaviour of the application using the CADP model-checker, and on the
other hand generate an executable ProActive/GCM code corresponding to the designed system.

\lstdefinestyle{encore}{
  language=java,
  basicstyle=\tt\color{black},
  keywordstyle=\tt\bfseries\color{blue},
  commentstyle=\it,
  aboveskip=1ex,
  belowskip=1ex,
  tabsize=2,
  columns=fullflexible,
  xleftmargin=1ex,
  resetmargins=true,
  showstringspaces=false,
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  escapeinside=??,
  morekeywords={assert,exclusive,stable,atomic,immutable,locked,lockfree,unsafe,trait,require,async,finish,def,consume,finish,async,J,S,var,val,shared,safe,subordinate,I,let,in,excl,then,else,wlock,wlocked,rlock,rlocked,read,thread,synchronise,linear,unpack,pack,subord,sync,bound,as,encaps,active,passive,requires,Fut,suspend,await,get}
}

\lstnewenvironment{ecode}{\lstset{style=encore}}{}
\lstnewenvironment{ecodep}[1][numbers=left]{\lstset{style=encore,#1}}{}
\newcommand{\ec}[1]{\lstinline[style=encore,basicstyle=\tt]@#1@}
\newcommand{\Upscale}{\textsc{UpScale}\xspace}

\subsubsection{Encore}\label{sec:encore}

\paragraph{General presentation} Encore \cite{ref:encore}
is a general purpose parallel language based on active objects
developed since 2014 by an
EC-funded research project.\footnote{EU project FP7-612985 \Upscale:
From Inherent Concurrency to Massive Parallelism through Type-based~Optimisations.} 
The language has been designed to excel at scalability and rests on four pillar concepts: 
\textit{parallel by default} using the
active object paradigm for coarse grain parallelism, \textit{independent
local heaps} to promote data locality, \textit{type-based synchronisation directives}
provided by a novel capability system and \textit{coordination of parallel
computations and low-level data parallelism} via parallel combinators. 
%\TW{Make sure to revisit all of these pillars properly in the text. (And that it is easy to find them.)}
On top of these key ingredients, Encore stays loyal to the object-oriented paradigm
where active objects and futures can be seen as normal Java objects and,
instead of interfaces, provides a trait system \`a la Scala.

\paragraph{Language description}
In Encore, active objects have their own thread of control and communicate with each 
other by asynchronous message sends. Messages are appended to the receiver's message queue
and processed one at a time in FIFO order. % This property linearises
% the execution of the active object. Tobias: I removed this since it did not make sense
Active objects act like ``plain old Java objects'', except that
they have an asynchronous interface. Active objects encapsulate
passive objects, and operations on passive objects are synchronous. 
%
Each active object owns a local heap on which it can
allocate passive objects. Ownership in this context means that the
actor is responsible for keeping track on foreign dependencies on
passive objects on its local heap, and eventually deallocate them. 
The passive objects on a local heap may be shared with
other active objects, or transferred, but are susceptible to data-races.
%
Encore is \emph{parallel by default}, as active objects are lightweight and provide coarse-grain parallelism,
i.e., an active object cannot work on multiple method calls at the same time.
%
To create fine-grain parallel computations Encore supports the creation of tasks.
A task (\ec{async \{ e \}}) contains a body \verb|e| that is asynchronously executed
and, whose return value is immediately a future. Tasks are more lightweight
that actors (memory-wise) and increase the level of asynchrony on the system.

Method calls on active objects and spawning of tasks immediately return a future value
that, unlike ProActive, has the explicit type \ec{Fut t}, for
some \ec{t}. Futures support future chaining,
and both \ec{get} and \ec{await} operations similar to ABS.
%
These operations are explained in detail later (in \emph{Cooperate Scheduling of Active Objects}).

The example below (Fig.~\ref{fig:encore:example}) shows a buffer implementation
using features of the Encore language.
%
\begin{figure}[ht]
\begin{parcolumns}{2}
\colchunk{
\begin{lstlisting}[style=encore, firstnumber=1, lastline=14,numbers=left,xleftmargin=4ex,escapechar=\%,basicstyle=\small\tt,columns=fullflexible, showlines=true]                                                           
trait TQueue {
  requires top
  
  def enqueue(x: int): void
    this.top = new Data(x, this.top)
}

passive class Data { 
  ... 
}    

passive class Queue : TQueue {
  top: Link
}  
\end{lstlisting}
}
%
\colchunk{
\begin{lstlisting}[style=encore, firstnumber=14,numbers=left,xleftmargin=4ex,escapechar=\%,basicstyle=\small\tt,columns=fullflexible]                                                           
class Buffer
   queue: Queue
   
  def init(): void
    this.queue = new Queue()
    
  def put(item: Data): void
    this.queue.enqueue(item)

class Main
  def main(): void
    let buffer = new Buffer()
        data = new Data(21)
     in buffer ! put(data)
\end{lstlisting}
}
\colplacechunks
\end{parcolumns}
\caption{\label{fig:encore:example}Example of a Buffer implementation that uses traits, active and passive objects in Encore.}
\end{figure}
%
In this example, the \ec{Buffer} class is an active object with
a private \ec{queue} of type \ec{Queue}, which is a passive class. \ec{Queue}
implements the \ec{TQueue} trait (line 12). A trait can be
thought of as a Java interface with defined methods and may require some attributes to be present
in implementing classes. For instance, \ec{TQueue}
requires the existence of an attribute \ec{top} and provides the implementation
of the \ec{enqueue} method (lines 1 -- 6). Any passive object of type \ec{Queue} has an implementation
of methods defined in the traits it extends, in this case, the \ec{enqueue} method. The \ec{Main}
class is the entry point of the program and creates an active object (\ec{buffer}, line 25) and
some data (line 26); the method call (line 27) \ec{buffer ! put(data)} is a
uni-directional message send, i.e., it does not
return a future. If it was rewritten as \ec{buffer.put(data)}, it would return a \ec{Fut void}
value, which would allow the called to detect when the put message is finished processing on the receiver
side. 


\paragraph{Cooperative Scheduling of Active Objects}

Encore supports versions of the \ec{suspend}, \ec{get}, and
\ec{await} statements similar to ABS. In contrast with ABS, Encore
supports using suspend and await anywhere in a program, as opposed
to ABS which only permits calling these operations inside active objects.
\LUDO{I am still not sure what is meant here: in abs all objects are kind of  active 
no?}.
This is due to underlying  technical reasons
described in the implementation sections of Encore and ABS
\TW{Make it so! :) Encore threads yadayada.}. Encore's \ec{await}
statement is somewhat limited in relation to ABS's as it only
supports awaiting the fulfilment of a future, and not any boolean
condition and differs from ProActive's \ec{await} . \LUDO{perhaps link here to the ProActive ABS backend that encodes await on 
conditions by await on future} Encore's support for future chaining however makes
\ec{await} less useful as chaining allows triggering arbitrary operations on the fulfilment of future. 
The type signatures of the operations
are found below:

\begin{center}
\begin{tabular}{rcl}
\ec{suspend} & \ec{::} & \ec{void -> void} \\
\ec{    get} & \ec{::} & \ec{Fut t -> t} \\
\ec{  await} & \ec{::} & \ec{Fut t -> void} 
\end{tabular}
\end{center}

A pictorial overview of the semantics of \ec{suspend}, \ec{await}
and \ec{get} is found in Figures~\ref{fig:enc-suspend} and
\ref{fig:enc-get-await}. It shows how the operations may involve
\emph{descheduling} an object (removed from round-robin scheduler
queue), \emph{blocking} (no processing of other messages in the
object's queue), and whether a resuming message is prepended or
appended to the object's queue.


% \LUDO{TBDiscussed: rewrite the explanation on get/suspend so that it is closer
% to ABS} Encore currently provides three operators for controlling execution flow
% within and across active objects: \verb|suspend|, \verb|get|, and \verb|await|.
% \verb|suspend| suspends the current message processing, appends a resuming
% message to the message queue and passes control to the next message in the
% message queue (which could be the resuming message if the message queue was
% empty). \verb|get| takes a future as its operand, returns the value in the
% future. In other words, \verb|get| has type: \verb|get :: Fut t -> t|. Depending
% on the status of the future, resolved or not, the control flow change caused by
% \verb|get| is different: if the future is resolved, \verb|get| doesn't alter the
% control flow at all; on the other hand, if the future is not resolved yet,
% \verb|get| would caused the calling active object to be de-scheduled until the
% future is resolved. The active object would be resumed at the exactly same
% execution point except that the future is resolved, and the call to \verb|get|
% returns. Because \verb|get| would stop the whole active object, which might not
% always be desirable, \verb|await| is provided so that only current message
% execution is paused, but the active object could still process other messages in
% the message queue. When the future becomes resolved, a special message is sent
% by the resolver of the future to the active object so that the paused message
% execution could resume. Let's see how each of them behaves in the following
% example. \LUDO{what does the next sentence change?}(To keep things simple, it's
% assumed to be running on a single-core machine or specified to use single
% Operating System thread.)

% \begin{lstlisting}[
%   basicstyle=\footnotesize,language=Haskell,
%   multicols=2,
%   numbers=left,xleftmargin=4ex,escapechar=\%
% ]
% class Agent
%   def print_1_3() : void {
%     print 1;
%     suspend;
%     print 3;
%   }
%   def print_2() : void {
%     print 2;
%   }
%   def produce() : int {
%     print 2;
%     3;
%   }
%   def print_1() : void {
%     print 1;
%   }
%   def self_produce() : void {
%     let
%       it = this
%       r = it.produce()
%     in {
%       await r;
%       print get r;
%     }
%   }
% class Main
%   def test_suspend() : void
%     let
%       a = new Agent
%     in {
%       a.print_1_3();
%       a.print_2();
%     }
%   def test_get() : void
%     let
%       a = new Agent
%       ret_future = null
%       ret = 0
%     in {
%       ret_future = a.produce();
%       print 1
%       ret = get ret_future;
%       print ret;
%     }
%   def test_await() : void
%     let
%       a = new Agent
%     in {
%       a.self_produce();
%       a.print_1();
%     }
%   def main() : void {
%     -- call the other two methods like this
%     this.test_suspend();
%   }

% \end{lstlisting}

% The \verb|Main| class contains three methods for showing the semantics of each
% operator, and the \verb|main| method, the entry point of the program. In the
% \verb|test_suspend| method, two messages, \verb|print_1_3| and \verb|print_2|,
% are sent to the agent one after the other. The \verb|suspend| operator in
% \verb|print_1_3| will suspend the execution of \verb|print_1_3|, and move the
% control to the next message in the queue, which is \verb|print_2|. Once
% \verb|print_2| is finished, the control will go to the next message, which is
% the resuming message created due to \verb|suspend| call in \verb|print_1_3|.
% Therefore, the output would be 1, 2, 3. In the
% \verb|test_get| method, the \verb|produce| call returns a future, and
% calling \verb|get| on that future (which can be guaranteed to be un-resolved
% while running in the single threaded settings) blocks the \verb|test_get|
% method until the future is resolved. Therefore, the output is 1, 2, 3, separated
% by new lines. In the \verb|test_await| method, two messages, \verb|self_produce|
% and \verb|print_1|, are sent to the agent. The agent processes them in
% order, but in \verb|self_produce|, \verb|await| is called on an un-resolved
% future (which is guaranteed to be un-resolved due to the same reason as above),
% which pauses the execution, and moves the control to the next message, which is
% \verb|print_1|. After that, the \verb|produce| message, created inside
% \verb|self_produce|, is processed, which resolves the awaited future, and
% \verb|self_produce| is resumed. Therefore, the output is 1, 2, 3.

\begin{figure}[t]
  \centering
  \includegraphics[scale=.4]{pictures/enc-basic}
\qquad
  \includegraphics[scale=.4]{pictures/enc-resume}
  \caption{(Left) An Encore scheduler, local to one core. A
    scheduler has a queue of active objects with non-empty message
    queues. (Right) The state after the execution of \ec{suspend} in the left 
    configuration.}
  \label{fig:enc-suspend}
\end{figure}

\begin{figure}[t]
  \centering
%  \includegraphics[scale=.4]{pictures/enc-get-await-fulfilled}
  \includegraphics[scale=.4]{pictures/enc-get-unfulfilled}
\qquad
  \includegraphics[scale=.4]{pictures/enc-await-unfulfilled}
  \caption{(Left) An Encore program executing a \ec{get} on an
    unfillfilled future. This causes the currently executing
    active object to be descheduled, only to be rescheduled after
    the future has been fulfilled. Once the object gets to the
    head of the scheduling queue after this point, the \ec{get}
    operation returns and the program continues from this point.
    with non-empty message queues. (Right) An Encore program
    executing an \ec{await} on an unfillfilled future. In
    difference with \ec{get}, the active object is only
    descheduled if it does not have any messages in its message
    queue, and once the future is fulfilled, the resuming method
    is placed at the end of the active object's queue.}
  \label{fig:enc-get-await}
\end{figure}


\paragraph{Parallelism Inside Active Objects}

Active objects provide coarse grain parallelism via futures but do not offer
many high-level language constructs for low-level coordination of these parallel computations.
In order to express complex coordination workflows, such as pipeline and
speculative parallelism, Encore incorporates
\parT{} collections and associated parallel combinators.
A \parT{} collection is an abstraction that can contain asynchronous computations,
but it represents more than a mere container, as \parT{} can be seen as a handle to these
asynchronous computations, exploited via the combinators.
%
%The \parT{} collection is opaque---it
%does not allow observing the number of on-going computations, and
%whether individual computations have finished or not. 
%
\parT{} collections have the type \texttt{Par t} (\texttt{t} is a polymorphic type)
and can contain values and asynchronous computations
represented by futures.
%, but it does not create new threads,
%as that may have a negative impact performance-wise.
%
%\TW{This sentence is not possible to understand:} Elements of a collection can be normal values and
%future values which can be \textit{lifted} to a parallel collection via
%\texttt{liftv :: t $\to$ Par t} and \texttt{lift :: Fut t $\to$ Par t} respectively.
%
%\TW{Rather than just going through the combinators, say how a
%  parallel collection is created (that's not visible in the text
%  now), and how it can be ``extended'' by additional combinators.
%  A picture of a pipeline and its combinator stages would be good!
%  Once the reader understands the what ParT types do, you can list
%  the other combinators because then she is ready to think about
%  them in a constructive manner.}

Futures created from tasks or methods calls on active objects can be lifted
to the \parT{} abstraction via the \emph{lift} combinator (lift :: Fut t $\to$ Par t);
multiple \parT{}s can be treated as a single one with
the $\|$ construct ($\| :: Par \;t \to Par \;t \to Par \;t$), which takes two \parT{}s
and returns a new \parT{} collection. 
\LUDO{could you say: which merges two \parT{}s into a single one? this would be more 
informative}
This construct does not create new threads,
but exploits the inner parallelism within the \parT{}, spawning new tasks when there
is an optimisation opportunity. \LUDO{this creates no new thread but creates new task? 
how? what do you mean? aren't the thread and tasks already created? ... }

The \parT{} combinators are informally explained with the example from Fig.~\ref{fig:encore:factorisation},
which computes the LU and Cholesky factorisation in parallel, inverting these matrixes asynchronously,
getting the diagonal of the first asynchronous computation that finishes inverting the matrix
and terminating the remaining computations.
%
The LU and Cholesky factorisation are performed in parallel, spawning tasks (lines 26 -- 27), and the
obtained futures are lifted and grouped into a \parT{} abstraction (line 28).
%
The inversion of the matrix is performed asynchronously, using the \emph{bind} ($>>=$) combinator.
This combinator ($>>= :: Par \ t \to (t \to Par\ t') \to Par\ t'$) receives as second argument 
a function that will be applied asynchronously to the items in the
\parT{} collection (first argument), therefore creating pipeline parallelism. 
%
Afterwards, with the prune combinator ($<< \; :: (Fut \;(Maybe \;t) \to Par \;t') \to Par \;t \to Par \;t'$),
the \verb|getDiagonal| function is applied to the first computation that returns the inverted matrix,
and the remaining computations are terminated --- safely stops speculative work.

\begin{figure}[t]
\begin{parcolumns}{2}
\colchunk{
\begin{lstlisting}[style=encore, firstnumber=1,numbers=left,xleftmargin=4ex,escapechar=\%,basicstyle=\small\tt,columns=fullflexible]                                                           
def luFact(Matrix m): [Factor] {
    ...
}                                                                              
                                                                                
def choleskyFact(Matrix m): [Factor] {
    ...
}
                                                                                
def matrixInv(f: [Factor])
        : Par [Matrix] {
    ...
}

def getDiagonal(Fut (Maybe [Matrix]))
        : Par [Factor] {
    ...
}
\end{lstlisting}
}               
%
\colchunk{
\begin{lstlisting}[style=encore, firstnumber=18,numbers=left,xleftmargin=4ex,escapechar=\%,basicstyle=\small\tt,columns=fullflexible]                                                                                                                                           
class Main {

  def getMatrix(): Matrix {                                                    
      ...                                                                      
  }                                                                            

  def main(): [Factor] {
    let mtx = this.getMatrix()                                            
        fLU = async luFact(mtx)
        fChlsk = async choleskyFact(mtx)
        par = (lift fLU) || (lift fChlsk)
    in {
        getDiagonal << 
            (par >>= matrixInversion)
    }
  }
}
\end{lstlisting}
}
\colplacechunks
\end{parcolumns}
\caption{\label{fig:encore:factorisation}Data-pipeline and speculative parallelism for matrix factorisation in Encore. }
\end{figure}


%The available combinators that operate on the \parT{} collection are:
%\LUDO{the explanations below are less clear than the rest of the text, I also think they 
%should be better justified from a high-level point of view: actor constructs will be 
%explained in details before but not those ... to be discussed, but a few comments}

%\textbf{$| \; :: Par \;t \to Par \;t \to Par \;t$,} read as \textit{par} combinator, that creates a new
%\parT{} that holds the computations from the first and second expression. \LUDO{do you 
%mean it concatenates the 2 constructs? or does this create parallelcomputation? how?}

%\textbf{$>< \; :: Par \; t \to Par \;t \to Par \;t$,} read as \textit{otherwise} combinator,
%that executes the first expression until the returned value is known and, if empty,
%starts execution of the second expression. \LUDO{Now I am lost, I thought ParT was data 
%collection ... is it an expression instead?}
%
%\textbf{$>> \; :: Par \; t \to (t \to t') \to Par \; t'$}, read as \textit{sequence} combinator,
%applies the function to all elements of the \parT{} collection asynchronously -- creating
%parallel pipeline parallelism-- and returns a new \parT{} collection. \LUDO{is it a map 
%or is it something else? a pipeline is one (stream of) data and a set of functions to 
%evaluate, I am not sure this is the case here}
%
%\LUDO{I will have a look at the end of the paragraph once the above has been resolved}
%
%\textbf{$<< \; :: (Fut \;(Maybe \;t) \to Par \;t') \to Par \;t \to Par \;t'$}, read as 
%\textit{prune} combinator,
%starts execution of both expressions in parallel. As soon as there is a value available
%in the second expression, this on gets passed to the first expression and all the remaining
%computations from the second expressions are ignored. 

The \parT{} collection and its combinators have been designed to perform
operations asynchronously without stopping the current thread of execution.
This creates a collection that never blocks and executes operations on demand,
when the values are available. There are more combinators available \cite{ref:encore}
but we have highlighted the most important ones. \LUDO{it would be good to conclude this 
paragraph by two sentences exaplining how parallel combinators integrate  well with the 
active object model (concurrency inside a single task at least)}


\paragraph{Degree of Synchronisation}

Like in ABS and in ASP, in Encore, synchronisation results of message passing and 
synchronisation on futures.  \ec{get} and  \ec{await} provide the same 
synchronisation possibilities and the same control on futures as in ABS: blocking until 
the future is resolved or releasing the current thread if the future is not resolved. One 
more original asynchronous constructs in Encore is the future chaining operator 
($\rightsquigarrow$).
 $\rightsquigarrow$ registers a callback
(anonymous function, aka lambda) to the future, continue processing the rest of
the message. The callback would be called with the future contend as the
argument when the future is resolved. This construct allows chaining operation on futures 
without creating synchronisation, it is kind of similar to the default behaviour of 
futures in AmbientTalk \TODO{add a citation if needed} except that it is more explicit 
and easier to control for the 
programmer. It is also a way to mimic 
 transparent first-class 
futures of ASP except that the callback request is only created when the future is 
resolved.

%\LUDO{I changed this paragraph to better relate with the other languages (perhaps too 
%long or too much references), the previous version is commented if you want to recover 
%some or all the text}

%In Encore, there are two ways for synchronisation, message passing, and
%synchronisation by futures. From the message passing perspective, each method
%call on active objects could be interpreted as a message transmitting, and
%different active objects could communicate and synchronise with each other using
%message passing, just like Erlang. The second approach, which is more akin to
%Object-Oriented style, is using futures. Each non-void method call on active
%objects returns a future, denoted as \verb|Fut| in Encore, and three operations
%are defined on futures, \verb|get|, \verb|await|, $\sim\sim>$ (future chaining).
%The first two operators would pause the current method execution until the
%future is resolved (fulfilled). The difference between them is the former blocks
%the whole actor, as discussed and explained in the previous section on flow
%control in Encore. On the other hand, $\sim\sim>$ would register a callback
%(anonymous function, aka lambda) to the future, continue processing the rest of
%the message. The callback would be called with the future contend as the
%argument when the future is resolved.

\paragraph{Degree of transparency}

Futures are denoted as \ec{Fut} in Encore, so non-future variables
and future variables are distinguished statically, e.g.\ \ec{int}
vs \ec{Fut int}. In other words, developers must decide which
variables are given which type, and \ec{get} could be use to
convert future type to non-future type (if the content of the
future is non-future type, i.e.\ not something like \ec{Fut Fut
  int}) by extracting the content explicitly. Both in future
management and with parallel combinators, the programmer is
exposed to the concurrent computations being handled in Encore.
However, especially when using future chaining or parallel
combinators, the scheduling and ordering of operations is handled
automatically, and the programmer only expresses concurrency from
a high-level point of view.

\paragraph{Data sharing}

In Encore, active objects are protected by their own thread of control
while passive objects are protected by a \textit{capability} type.
Encore's type system sees passive objects as resources that
are protected by a capability that governs the kind of access to it \cite{ref:encore}. 
A capability is built from a trait and a \textit{kind}. The trait provides the interface-like
feeling of OOP languages while the \textit{kind} states the ``protection'' that
the interface provides. By changing a keyword, the \textit{kind}, the interface
changes the protection level. For instance, by changing the \textit{kind} from \textit{exclusive}
to \textit{lock-free}, the interface changes the protection from an actor-like to a lock-free implementation.

The Encore capabilities form the hierarchy seen in
Figure~\ref{fig:capa-tree}. Exclusive capabilities are local to
one thread only and linear capabilities may be transferred between
concurrent computations without data-races of dynamic means of
concurrency control. Shared capabilities may be shared across
concurrent computations because these capabilities encapsulate
some means of protection against concurrent accesses: pessimistic
capabilities like locks and actors serialise computation;
optimistic capabilities like STM and lock-free capabilities allow
parallel operations but uses some form of roll-back scheme when
conflicts are detected. Immutable and read-only capabilities are
always safe to access concurrently because of the absence of
writes. Finally, subordinate capabilities allow constructing
aggregates of objects whose data-race freedom stems from their
proper encapsulation inside some other capability.

Encore capabilities are created from traits, and importantly,
different traits in the same class can construct different
capabilities. This allows a substructural treatment of objects,
e.g., taking a pair and splitting it into two disjoint halves,
which can be pointed to and modified separate of each other. One
of the more experimental features of Encore, which is far from
fully explored, is the combination of certain capabilities to
express e.g., an active object with a partially synchronous
interface (like a priority channel), or active objects which are
encapsulated inside other active objects to create constrained
active object topologies.

\begin{figure}[t]
  \centering
  \includegraphics[scale=.33]{pictures/capabilities}
  \caption{Encore capability hierarchy.}
  \label{fig:capa-tree}
\end{figure}

\paragraph{Formalization and semantics}
The Encore concurrency model has been formalized (\cite{ref:encore}) using
a small step operational semantics. Parallel combinators have been formalized as well
including a soundness proof with the implicit task parallelism model \cite{encore:parallel-combinators-thesis}.
\NOTE{Elias might have a soundness proof for the Capability system?}

\paragraph{Implementation: programming and execution support}
Encore is a relatively new programming language and there is still some
limited tooling support. However, the Encore compiler can emit a
highly understandable C code.
This C code can be analysed and debug using any available tool that
works with the C11 standard. 

The currently supported tools are:

\begin{itemize}
\item Emacs and Atom editors with syntax highlighting and compiler error support
\item Support for GDB / LLDB interactive debugger
\item Vagrant support for rapid installation of Encore in a virtual machine
\end{itemize}

\section{Implementation of active objects}
In this section, we take interest in the implementations of the different active object programming models described in the previous section. In practice, the implementation of a programming language can be provided from scratch or can also be given as an API in a more mainstream language. Another way to implement an active object language is to systematically translate any of its programs into another language; this is possible through a language backend: a translator that captures the semantics of the source language. The advantage of this solution is that an active object language can then have several independent backends targeting different execution platforms and, by this means, fitting the largest needs. 

\subsection{Dimensions of Comparison between Implementations}
In any language implementation strategies, several points need to be considered regarding the language runtime and how the implement efficiently the language semantics. We will use those points to compare active object implementations. 
For each active object implementation, we want to address \TODO{update number of points} five points, among them: how threads are created and handled to achieve the right scheduling, how object are referenced throughout an active object-based application and how objects and data are effectively shared. We describe them below.

\paragraph{Thread creation and scheduling}
% Virtual threads %
The active object languages that are implemented on top of an existing programming 
language have to comply to the constraints given by the underlying programming language. 
In the case of multi-threading, some underlying programming language feature light thread 
whereas having something different than a physical thread is impossible in other 
programming languages (e.g. Java). In this case, one can consider implementing light 
threads on top of the underlying programming language. Featuring light thread is also 
crucial if the goal of the given active object language is to scale in the number of 
active objects located on the same machine. 
% Continuations %
This notion of virtual threads is particularly important when implementing an active 
object language which features cooperative scheduling of requests: a thread  must stop 
executing when a request is paused and must resume afterwards. This is a programming 
challenge when it comes to implement this behavior in programming languages that do not 
support thread serialization, like Java. 

Concerning this dimension, the following questions are raised in general: how active 
objects and requests are mapped to threads? Should the thread be a physical thread or a 
virtual thread implemented on top of physical threads? Depending on these choices, some 
scheduling between different active objects or different requests might have to be 
considered.

%how active objects are mapped to threads and the scheduling levels within an object and 
%BETWEEN objects

\paragraph{Data sharing and object referencing}
This point answers to the question ``how objects are shared between active objects?'' 
Active objects encapsulate their state such that active objects are independent from each 
other.
By principle, active objects prevents race-conditions by allowing a single entity (a cog 
or an active object) access to it. This principle partitions the memory which limits or 
prevents data-race-conditions.
 In practice, this requires to distinguish between objects that can be remotely 
accessed (by asynchronous invocation) and objects that has to be copied when exchanged 
between active objects.
Accessing objects in another activity might involve additional communications and request 
treatment 
delays, while copying objects might lengthen the initial request invocation because of 
the serialisation time but fastens the request treatment. Handling copied objects also 
involves coherency issues. 
%Different languages take different choices on which objects are copied and how the copy 
%and the referencing of objects is handled.
Some implementation also create a pool of immutable objects that are easier to share 
without risk of data-race-condition. 

To communicate, objects must have a way to reference each other. In particular, to send a 
request to an active object, one must have a reference to it and be able to access it. 
In distributed settings, an efficient active object implementation cannot rely on a 
global shared memory, because it would be too costly. 
The ability to reference and interact with remote object raises even more questions in a 
distributed setting.

How to efficiently and safely share information 
between active objects is still an on going challenge and different kinds of data sharing 
strategies have been proposed in active object implementations.


\paragraph{Error handling}
Among non-functional features an active object implementation can provide, we can mention 
how errors are handled. 
In general, the handling of errors is easier in a sequential program where the context of 
an operation is known. Indeed, in a concurrent setting, if a task raises an error, this 
will have to be reported to another task that is willing to interact with the first one. 
When the second task is informed of the error it is difficult to react properly as the 
conditions that raised the error are not fully known.
 In the case of distributed 
implementations, exception handling is more complex as some of the exception might reveal 
failure of nodes or communications. Additionally, active object implementations can offer 
recovery or rollback mechanisms to allow  systems to recover after one or more active 
objects were lost. 
%This support is particularly relevant for distributed active object implementations, 
%because the failure rate is proportional to the degree of distribution. 

\paragraph{Garbage collection}
Garbage collection is another non-functional feature that an active object implementation 
should support. This point is important for the active object applications to scale and 
to be perennial. On most cases, the garbage collecting strategy must be implemented to 
fit a particular active object language. Even for active object languages built upon an 
underlying language with garbage collection, this language can only provide a partial 
garbage collection. Indeed, the true question in the case of active object 
implementations is when active objects are not needed any more, since they can receive 
request unexpectedly and from anywhere.
Again, the question is even more complex in the case of distributed implementations because reference counting is scattered.

Below, we review active object implementations, whether they rely on a translation to 
another language, have their own runtime, or are implemented as a middleware layer, 
according to the dimensions of comparison defined above.

\subsection{Java 8 backend for ABS}\label{sec:implem-java-8}
The Abstract Behavioural Specification (ABS) language allows several programming paradigms and design patterns offering both a functional model as well as an object-oriented model. ABS has a similar syntax to Java enhanced with several new constructs and annotations that allow design of distributed applications for grid and cloud environments. Annotations can include custom defined schedulers to ease the development of batch systems and workflows. The main goal of the Java 8 backend for ABS is to translate these models into production code to be executed in a parallel or distributed environment. The challenge of this implementation is to generate the real memory structures and execution instances while being aware of the possible resource limitations, communication bottlenecks or latencies and performance, as these requirements are not easily observable at a modeling level.  Active Objects in ABS are name Concurrent Object Groups (COG) and contain a set of smaller objects that run sequentially on one instance of execution.

\paragraph{Thread Creation and Scheduling}
In ABS, the core language contains constructs for the two finest levels of granularity in parallel computing which are scheduling method calls within an object and scheduling object execution within a task. Cooperative scheduling was a major challenge to implement before Java 8, as ABS cooperative scheduling required in Java the need to use Threads to map each method call and a Thread pool to map each object. Therefore a construct of an asynchronous method call requires the creation of a new Thread inside a thread pool along with the start of this thread executing the method. Our research question results from the fact that each object is supposed to execute on one thread. In Java each object will be in fact a Thread pool containing the number of threads corresponding to the number of method calls invoked by the object to support that finest level of scheduling featured by ABS. This is done despite only one method running at most on that object. This underlying thread model significantly affects performance of an ABS-modelled application that is translated into Java code. This approach was used in a previous Java backend that creates a really large number of threads that occupy a large portion of the program's heap. 
\TODO{Justine: perhaps you should say before that it's the old way of implementing it, Vlad: fixed}.



\paragraph{Cooperative Scheduling of Active Objects using Java 8}
\par Java 8 new features allow wrapping of method calls into lightweight lambda expressions such that they can be put into a scheduling queue of an ExecutorService to which the running objects are mapped, significantly reducing the number of idle Threads at runtime.  The only drawback is that if a method call contains a recursive stack of synchronous calls, upon preemption this stack needs to be saved, a scenario which cannot be achieved through lambda expressions. To solve this issue we focused our research in two directions:
\begin{itemize}
	\item On every preemption, we calculate the continuation and enqueue the rest of the call into the message queue.
	\item On every preemption, we try to optimally suspend the message thread until the continuation of the call is released.
\end{itemize}
The first direction is still in progress, as we have a fundamental technical limitation in the JVM/compiler. It is difficult to ask the JVM to make the rest of a method turn into another method call in the object's message queue. The second direction however offers the following possible scenario that is illustrated in Figure \ref{abs8:coop}:

\NOTE{Justine: I numbered the list so that I can compare points in the ProActive backend}
\NOTE{Vlad: OK }
\begin{enumerate}
	\item Each asynchronous call/invocation is a message delivered in the corresponding object's queue.
	\item All objects in the same COG  compete for one Thread.
	\item A Sweeper Thread decides which task should be created and be available for execution from the available queues.
	\item A thread pool executes available tasks based on a work stealing mechanism.
\end{enumerate}  

\begin{figure}
	\label{abs8:coop}
	\centering
	\includegraphics[scale=0.5]{./Sched.png}
	\caption{Cooperative Scheduling Solution}
	\label{cd}
\end{figure}


The only challenge now is when an execution of multiple synchronous calls is preempted. This results in a call stack and context that need to be saved within a thread. To do this the executing thread from the pool is suspended and will compete again, upon release, with the other available tasks in the COG for selection by the Sweeper Thread to be made available to the thread pool. However the Sweeper Thread gives priority to live threads ready for execution against new messages that have yet to become threads.

\paragraph{Data Sharing and Object Referencing}
In the Java 8 backend for ABS, objects are organized into COGs, each running on one thread. When objects are created, they are assigned a new or existing COG and all invocations on the object are executed on the same thread. Data that is shared among distributed objects is passed through lambda expressions that are sent as serialized messages between the objects, therefore all data that passes in a distributed system needs to be serializable. When COGs reside on the same machine, for example applications that run on one multi-core machine,  all data is passed as arguments of lambda expressions or synchronous method calls in Java 8\TODO{Justine: can you say in which case? Vlad:fixed}. Objects can hold as inner fields references to any other object both belonging to the same COG or a different one. Similar to data sharing objects must be serializable to be transferred between distributed objects. Furthermore, the generated classes will be automatically loaded on all machines that require an object of a particular type, such that remote objects can invoke methods the serialized references that they receive. 

\paragraph{Active Object Scheduling Example}
To study the improvement given by Java 8 features, we benchmark a simple example that is illustrated in Figure \ref{abs8:ex}. In the example we have one COG that contains an object which receives a large number of messages which are stored in its queue. This message recursively calls a function that creates a large stack frame after which a message is sent to a different COG to run in parallel a CPU intensive trigonometric function. The object is then suspended to await the result of this function, resulting in a large stack frame that needs to be saved in order to allow the next message from the queue to run on the COG.  We vary the number of recursive calls, the total number of messages in the object's queue and the complexity of the function, in order to observe a comparison with the old Java backend. \TODO{Justine: question to everybody - would it be relevant to put a chart here?}

\begin{figure}
	\label{abs8:ex}
	\centering
	\includegraphics[scale=0.6]{./Cobai.png}
	\caption{Cooperative Scheduling Example}
	\label{cd}
\end{figure}


\subsection{ProActive}
The ProActive library is all written in standard Java and provides an implementation for 
the ASP programming model. 
By default, it uses the remote method invocation Java package to implement the communication layer between active objects, although other communication protocols are possible. 
This fact drives most of the implementation singularities mentioned below.

\paragraph{Thread creation and scheduling}
In ProActive, an active object with no compatibility rule defined will be associated to one Java thread to process the requests. 
Furthermore, there is a Java thread to handle request reception in each active object. 
Java threads are mapped to operating system threads, thus ProActive uses at least two threads per active object. 
But as ProActive features multiactive objects, this number can be higher, as the active object scheduler can create Java threads on the fly to process a compatible request. 
As Java threads are pretty heavy, the ProActive scheduler implementation puts a particular effort on optimizing thread usage. 
Firstly, a thread pool is instantiated at active object start-up to ensure a basic thread reuse policy. 
Secondly, when the number of threads is limited at the application level (through multiactive object annotations), the limit literally maps to Java threads, so a fine performance tuning can be achieved at the application level. 
Additionally, threads that are waiting for a future can be temporarily reused to process another request, by stacking another method call on it. 
This is also configurable at the application level. 
So to conclude, in ProActive thread creation and thread scheduling are almost completely exposed to the programmer, allowing him to have a great control on the performance of ProActive applications.

\paragraph{Data sharing and object referencing}
% LUDO: I shhortenned the part that was common with ASP
like in ASP, ProActive differentiates active and passive objects in a way that is much 
transparent to 
the programmer, except that passive objects are passed by copy when communicated between 
active objects while referneces between active objects can be shared and accessed from 
anywhere.
The first reason for this behavior is that ProActive is based on Java RMI, and Java RMI 
is based on parameters copy.
The second reason is because of distribution: affording a consistent distributed memory 
is too costly for HPC, which is the primary target of ProActive.
Of course data is shared between the several threads of a multi-active object.

This sharing pattern is thus naturally implemented using RMI and Java serialization that 
performs a deep copy of the parameters transmitted between objects. Future and active 
objects are implemented by a proxy that can be passed by referenced and is serialized as 
a reference (without any copy of other objects referenced). The serialization mechanism 
is also used to track the multiple references to the same future and to implement 
efficient future update strategies~\cite{HKRZ:Coregrid:2010}.
%
%In the case of active objects, they can be indeed shared between active objects but this 
%does not imply safety issues since request reception is centralized and request 
%execution 
%is controlled.
%Thus, one way to share an object globally is to turn it into an active object, but this 
%strategy should be unusually used for performance reasons.
%Another aspect of data sharing in ProActive applies within an active object. 
%Since ProActive implements multiactive objects, there is a potential a multi-threaded 
%environment in each active object. 
%Consequently, objects can be shared between several threads living in the same active 
%object. 
%Data protection must be then ensured by the programmer: either by compatibility 
%definition or by classical locking mechanisms.

%Once again, ProActive object referencing is driven by the underlying technology legacy. 
In RMI, remote objects are globally referenced in what is known as the RMI registry: a mapping from remote object names to remote object stubs, that can be copied and used anywhere in order to target the remote object. 
Networking communication is ensured by the rmi protocol. 
Consequently, in ProActive, all active objects are referenced in a RMI registry, and can be accessed this way from any other object communicating through the rmi protocol. 
For objects that are not active objects, traditional Java object references are used as 
they are only referenced within the same active object.
In conclusion, two types of object references exist in ProActive: global references, retrievable from the RMI registry, and local references, manipulable like classical references in programming languages. 

\paragraph{Error handling}
Distributed environments are prone to failures. 
Since ProActive targets distributed environments, a particular effort was made in producing robust ProActive applications by design. 
For that, the ProActive library includes two different error handling aspects. 
First, an exception chaining mechanism was developed on top of RemoteException, which is 
the basic RMI exception, in order to have a readable feedback when ProActive applications 
crash. Also a specific API has been developed to compensate for the difficulty to deal 
with asynchronous exception and the lack of control on the Java exception 
mechanism~\cite{CC-ECOOPWS2005}. 
Another aspect of the ProActive library consists in continuing the execution of the application in presence of some failed active objects, for example if a machine hosting some active objects of the application crashes. 
For this purpose, ProActive implements a  fault tolerance protocol specific to the active 
object semantics. It enables a set of failed active objects to restart  from the latest 
checkpoint. 
The checkpoints are taken per active object, based on the communications between them. 
This strategy is coupled with the logging of events received by the active object, to ensure a deterministic reexecution. 
The fault tolerant protocol is complete for applications that only feature mono-threaded active objects. 
It is under development for applications that feature multiactive objects.

\paragraph{Garbage collection}
Handling garbage collection in ProActive is deeply linked with how objects are referenced. 
Firstly, to ensure garbage collection of regular objects, no particular attention is needed, simply because the given Java garbage collector will take care of them. 
Indeed, as a regular object cannot be referenced by several active objects, it is guaranteed that no reference to this object exists in another JVM, so classical garbage collection is enough.
However, in the case of active objects, we cannot directly know if it still exists a reference to an active object, because many proxies of an active object can be disseminated throughout the network.
An adapted algorithm to garbage collect active objects was developed in ProActive. 
It is based on the detection of useless active objects (those that are idle and only 
referenced by idle active objects). This is detected as some form of common agreement 
based on the reference graph between active objects~\cite{CCH:Middleware07}. Garbage 
collection of passive objects can safely rely on the Java garbage collector.

\subsection{ProActive backend for ABS}
The ProActive backend for ABS is another translator that generates the ProActive code 
corresponding to an ABS program. The goal of the ProActive backend for ABS is similar to the one of the Java 8 backend for ABS: turn ABS models into production code. The difference is that the Java 8 backend targets optimized performance, this why it is still under development, whereas the ProActive backend provides a fully working execution of ABS models in distributed environments, with less optimization than expected with the Java 8 backend. The ProActive backend aims at \emph{simulating} all ABS concepts 
with ProActive, since the two languages are based on different active object models. The 
main challenges in the translation are (i) how to efficiently support object groups in 
ProActive where there only exist active and passive objects, (ii) how to address objects 
in the translation since objects are passed by reference in ABS and by copy in ProActive, 
and (iii) how to simulate cooperative scheduling with multi-threading controlled through 
annotations. We first present object referencing aspects as scheduling is   
dependent on the set of objects that are decided to be active.

\paragraph{Data sharing and object referencing}
As in ABS all objects should be accessible from all others, one could think that implementing all objects with a ProActive active object would meet the requirement. However, in practice, this is hardly manageable: a ProActive active object is associated a plain Java thread. Thus, this solution would inevitably lead to a substantial memory consumption and context switch overhead.
In the code generated by the ProActive backend, only the COGs are active objects, and thus are the entry points to all the objects they contains. Thanks to this hierarchy, objects other than COG objects remain passive, which preserves the performance of the ProActive backend.
Thus, we have a two-level hierarchical indexing of objects:
\begin{itemize}
\item A first level of network-wide accessible COG objects. This mechanism is integrated in ProActive as it is based on RMI~\cite{Wollrath:1996:DOM:1268049.1268066}.
\item A second level of locally accessible objects. This mechanism is implemented in the COG class using a map from object identifiers to object references.
\end{itemize}
This classification implies that the application always manipulates local references. The 
translation introduces indirections through COGs that can be accesed by remote references.
This configuration of objects is illustrated on Figure~\ref{fig:ABS-to-PA-new}, where a new COG is created through a new \code{Server} object.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{pictures/ABS-to-PA-new.png}
	\caption{Object organization given by the ProActive backend for ABS when a new COG is created}
	\label{fig:ABS-to-PA-new}
\end{figure}
%% RM
In the ProActive backend for ABS, the principle to translate an asynchronous ABS call is 
thus to run a generic asynchronous method call on the COG of the targeted object and then 
let the generic method of the COG retrieve the targeted object, thanks to a unique 
identifier, and run the desired method on it by reflection. Figure~\ref{fig:ABS-to-PA-call}
%% RM
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{pictures/ABS-to-PA-call.png}
	\caption{Translation of an ABS asynchronous method call in ProActive}
	\label{fig:ABS-to-PA-call}
\end{figure}
In ProActive, passive objects are not shared between active objects. Therefore, when a passive object is a parameter of a remote method call, it is copied, whereas in ABS parameters of method call are manipulated by reference.
So when in the translation we run a generic method call on a COG, all the parameters are 
copied. For the identifier of the targeted object and the name of the method to run, this 
is not a problem because they are immutable variables, similarly for parameters that are 
primitive values. Concerning 
the parameters of the method to run that are reference to ABS objects the different 
parameter passing semantics could raise a problem.
%% RM
However, 
any access to those objects is done through an asynchronous method call on the copy of 
method parameters, those invocations end up in calling the unique original 
version of the created object and their hosting COG. Thanks to this mechanism, the 
initial data sharing of ABS is correctly 
simulated even with the no-sharing philosophy of ProActive because the copied data is 
only used as a reference to the original object.
In the end, in the ProActive translation of ABS, when we copy an object from one node to 
another, we only need the identifier of the object, to be able to retrieve it in the 
right memory space, and a reference to its COG, that is globally accessible. All the 
others attributes of the objects can actually be omitted since they will not be used when 
copied. This observation allows us to optimize object copy and reduce the amount of 
information that is sent through the network. In summary, the ProActive backend generate 
 programs that copy few data but incur more communications 
than native ProActive applications. Compared to the Java 8 backend for ABS, the ProActive backend does not distinguish whether COGs are located on the same machines to optimize communications, however this is not a problem since copies are always lightweight. Also, in the ProActive backend class loading is handled by RMI wheres it is implemented from scratch in the Java 8 backend.

\paragraph{Thread creation and scheduling}
Multiactive objects provide several mechanisms to control the scheduling of requests. The right tuning of those controls allows the ProActive backend to simulate the behavior of ABS cooperative scheduling. 
%In general, we create more threads than supposedly in ABS because, when a thread is handed over to another request in ABS, in fact in the ProActive backend a thread is paused and another one is allowed to continue to simulate the same behavior.
More precisely, we can compare the ProActive backend and the Java 8 backend  on the four points explained in Section~\ref{sec:implem-java-8}. On points 1 and 3, the ProActive backend also delivers a message per asynchronous invocation, but a single request queue exist for all objects in a same COG, whereas in the Java 8 backend, there is a request queue per object. Point 2 is implemented differently from the two backends: the Java 8 backend ensures that all objects in the same COG compete for one single threads (which closely sticks to ABS models), whereas the ProActive backend creates one thread per executing (and paused) requests: it is actually the precise interleaving of several threads that ensures that only one request is progressing at a time; this strategy avoids implementing continuations in Java which is made difficult because of JVM constraints. Finally, on point 4 the Java 8 backend shares a thread pool for many COGs whereas in the ProActive backend each COG has its own thread pool (this avoids distinguishing whether COGs are distributed). In conclusion, the ProActive backend has to handle more idle threads than the Java 8 backend. We explain below in more details how we implement ABS preemption constructs in ProActive.

\smallskip
$\bullet$ \textit{Translation of ABS \code{await} statement on futures.}
%% RM
In ProActive multiactive objects, a way to control what happens when a thread switches in wait-by-necessity is to specify the kind of thread limit (in addition to the thread number) that is used by the considered multiactive object: a hard limit restricts the total number of threads existing for a multiactive object instance whereas a soft limit only restricts the number of threads that are active (not in wait-by-necessity).
Thus, in order to achieve the desired request scheduling, we configure the COG class and its \code{execute} method with multiactive object annotations.
%% RM
We declare that the generic method of the COG that executes all methods by reflection is 
compatible with itself, which means that many of such methods are allowed run in 
parallel. Additionally, we declare that this set of methods has a thread limit of 1, and 
that this limit is a soft limit (only thread that  are not in wait-by-necessity are 
counted in this limit). Consequently, there is only one active thread at a time that 
serveds requests in a COG multiactive object, so generic COG method calls do not run in 
parallel but interleave upon wait-by-necessity. Consequently, this multiactive 
configuration leads to a similar scheduling as ABS.
	
\smallskip
$\bullet$ \textit{Translation of ABS \code{get} statement.}
The translation of the ABS \code{get} statement would have been straightforward if we had 
not configured the COG class for the translation of the \code{await} statement. Indeed, 
by doing so, we have disabled the blocking aspect of a ProActive wait-by-necessity 
(enabling cooperative thread release). 
Consequently, what is required to translate the \code{get} statement is to temporarily go back to the initial behavior of a ProActive wait-by-necessity (i.e. blocking, like without annotations).
To do that, we temporarily switch the thread limit to a hard thread limit, so that no 
other thread can start or resume a request in the meantime. When the future is resolved, 
we set back the soft limit restore the cooperative scheduling behaviour. 
%% RM

\smallskip
$\bullet$ \textit{Translation of ABS \code{await} statement on conditions.}
Another usage of the ABS \code{await} statement is to use it followed by a condition. For 
translating this kind of \code{await} in ProActive, we wrap the condition evaluation in a 
method call that returns a future, and we synchronize on it afterwards. More precisely, 
for each such condition, we generate a corresponding method. Then, we translate the 
\code{await} statement by an asynchronous call to another generic method of the COG class 
that is specially devoted to execute waiting conditions, giving it all required 
parameters to do that. Again, we configure this method with multiactive annotations in 
order to allow several conditions to run in parallel. 
We allocate many threads for the evaluation of conditions and reserve through annotations 
one thread for the generic method execution.  
%% RM  
This way, we have a clear separation between the single thread used by application 
method, and the threads used by the condition execution method.
%% RM

In conclusion, all ABS constructs are sustained by the ProActive backend. We have also 
proven the translation to be correct. For that, we have used ASP with multiactive objects 
extensions, and we have showed that the ASP translation simulates all the rules of ABS 
semantics, and that all translated ASP configurations correspond to a valid ABS 
execution. In addition, we have performed two experimental evaluations on an ABS 
application translated with the ProActive backend, that show that (i) the translated 
application run in a distributed environment indeed outperforms traditional ABS program 
execution, and (ii) that the overhead introduced by the translation is always kept under 
10\% compared to a native ProActive application.


\subsection{Encore}
\TODO{clean up this introduction}
explicit bytecode operations?
particular accent on data sharing since lots of care has been put is designing an efficient data sharing in Encore.

Encore language implementation consists of two major components, a
Source-to-source compiler (from Encore to C) implemented in Haskell, and the
runtime system, which is based on the  runtime (implemented in C) used by
Pony.

\paragraph{Thread creation and scheduling}

On system start-up, the same number of schedulers are created as the number of
physical cores on the machine unless specified otherwise on the command line.
Each active object in Encore is assigned to an Encore thread (green thread),
which provides thread-of-control for the active object so that it could process
messages in the mailbox. Each scheduler is mapped to an OS thread, and it runs
in a loop of scheduling active objects until the whole program 
terminates. Each scheduler owns an active object queue, and in each iteration of
the loop, it performs three procedures. Firstly, pop an active object from the
beginning of the queue. Secondly, hand over the control to the active object so
that it can process messages in the mailbox if any. The active object can only
process one message in this step to ensure fairness. Thirdly, depending on if an
message is indeed processed in previous step, the active object is pushed to the
end of the queue \TODO{make this clearer3: what if no message is processed?}. In the 
second step, new active objects could be created, and
they are pushed to the end of queue belonging to the current scheduler as well.
Active objects can be transmitted from one scheduler to another because of work stealing, 
which \LUDO{Remove this (seems obvious to me)?
tries to achieve load-balancing, and} happens whenever a scheduler runs out of
active objects.

As described in Section~\ref{sec:encore}, processing of a
message could be paused in the middle, and be resumed when the request service is 
scheduled again. All information living
on the stack must be preserved for message processing continuation. Currently,
Encore runtime fetches a free stack page from the stack pool to process
next active object, this new stack page is used until next pause
happen. When message processing finished,
the stack page is collected and returned to the stack pool for future reuse.

\paragraph{Data sharing and object referencing} \LUDO{I think this paragraph needs proof 
reading and improvement: I tried to do the basic but more is needed}

When data (active and passive objects) is shared in Encore, no copying is
performed, so sharing large objects in Encore poses no performance issues. Consequently, 
sharing mutable passive objects
naively could introduce data race. Active objects are not subject to this
problem, for they have an internal thread-of-control. Mutable passive objects are
immune of this problem for oblivious reasons \LUDO{You mean immutable?}. Therefore, the 
most interesting
case is sharing mutable passive objects. Under capability system, a mutable
passive object could be of kind locked so that sharing it would not result into
data race. In a slightly more advanced usage scenario, we might want to share
part of the mutable passive object so that multiple active objects could work on
different part of the same passive object concurrently. The capability system is
work-in-progress, and not available in the compiler currently.

\paragraph{Error handling}

Currently, Encore doesn't have support for exception handling. Errors are
expressed using \verb|Maybe| type, and programmers could do pattern match on it
to cater for the erroneous cases.

\paragraph{Garbage collection}

Encore borrows the garbage collection part from Pony runtime with the necessary
extension to accommodate future values, which is not available in Pony. Garbage
collection consists of two parts, collecting active objects, which is covered in
\cite{ref:pony_actor_gc}, and collecting passive objects. %, which is discussion in
%a paper under submission.

\subsection{Erlang backend for Rebeca}
Execution of Rebeca models is done by translating Rebeca models to Erlang code 
\cite{DBLP:journals/scp/ReynissonSACJIS14}. Erlang is a dynamically-typed general-purpose 
programming language for development of distributed, real-time and fault-tolerant 
applications; it provides an actor-based concurrency model. Having the same concurrency 
model, translating Rebeca models to Erlang is realised by direct mapping of language 
constructs as shown in \cite{DBLP:journals/scp/ReynissonSACJIS14}.

\paragraph{Thread Creation and Scheduling}
Actors, as the only concurrent elements of Rebeca models, are mapped to processes in 
Erlang, concurrently executing elements which are extremely lighter than OS-level threads 
(more than 100,000 of them can be run in a single computer) \cite{Erlang-book}. The 
programming facilities of Erlang for developing concurrent applications (i.e., spawn, 
``!'', and receive) allow processes to create new processes and communicate through 
asynchronous message passing. This facilities are used for translating Rebeca models to 
Erlang codes without needing any modification. The generated processes are scheduled 
using the default scheduler of Erlang, called reduction-based scheduler.
%To model the actors scheduler of Rebeca, reduction-based scheduler of Erlang is used. Using reduction-based scheduler, an Erlang process is allowed to execute for a time slice of approximately 2000 reductions before a context switch occurs. A reduction corresponds to the execution of one of the Erlang's built-in functions.% So, a single reduction is not a constant value.

\paragraph{Data Sharing and Object Referencing}
Reserving a dedicated memory space for each process in Erlang, avoids having any shared 
object among processes. Upon communication, the sent message and its parameters must be 
stored in the message bag of the receiver actor which is located in its dedicated memory 
space. %So, a copy of the parameters is stored in the receiver's memory space. %This 
%%mechanism is called copying message passing of Erlang. 
\LUDO{the end is not clear to me}
Although, passing reference of actors using this mechanism does not result in sending the deep-copy of actors and only reference to actors are copied and are sent. The references to actors are allowed to be used for message sending not for sharing data of actors.

\paragraph{Error handling}
In the current version of Rebeca, there is no mechanism for exception handling and the 
programmer must rely on traditional messages to deal with errors. %So, modelers have to 
%handle exceptions manually. In other words, exception handling is not a special case in 
%Rebeca models and they must be implemented the same as ordinary parts of message servers 
%of actors.
%Currently, Rebeca doesn't have support for exception handling. 
%Codes for handing errors must be implemented the same as ordinary parts of message 
%servers of actors.


\paragraph{Garbage collection}
Rebeca borrows the default garbage collector of Erlang without any modification, which 
runs one garbage collector for each process. %It uses general garbage collection 
%algorithm (dividing memory to young and old generations) for each process. 
\LUDO{I do not understand the last two sentences}
This way, as each Rebeca actor is mapped to one Erlang process, we can say that per actor garbage collection takes place in translated codes. %In addition, the intra process garbage collection only analyses large processes (i.e., processes which consumes more than 20K of memory). So, 
In memory pressure, all of the Rebeca actors are analyzed to this aim as they are long-lived large processes.

\section{Lessons Learned and Conclusion}

%\input{survey-parts/introduction.tex}
%\input{survey-parts/example.tex}
%\input{survey-parts/conclusion.tex}

% Appendix
%\appendix
%\section*{APPENDIX}
%\setcounter{section}{1}
%In this appendix,...

%\appendixhead{ZHOU}

% Acknowledgments
%\begin{acks}
%The authors would like to thank Dr. Maura Turolla of Telecom
%Italia for providing specifications about the application scenario.
%\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{2015-active-objects-survey,biblio,envisage,rebeca}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012

% History dates
%\received{February 2007}{March 2009}{June 2009}

% Electronic Appendix
%\elecappendix

%\medskip

%\section{This is an example of Appendix section head}

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
